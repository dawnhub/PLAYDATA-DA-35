{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석 기반 토큰화의 문제\n",
    "\n",
    "-   형태소 분석기는 작성된 알고리즘 또는 학습된 내용을 바탕으로 토큰화를 하기 때문에 오탈자나 띄어쓰기 실수, 신조어, 외래어, 고유어 등이 사용된 경우 제대로 토큰화 하지 못한다.\n",
    "-   그래서 발생 할 수있는 잠재적 문제점\n",
    "    -   어휘사전을 크게 만든다.\n",
    "        -   같은 의미의 단어가 형태소 분석이 안되어 여러개 등록될 수있다.\n",
    "        -   ex) 신조어 `돈쭐` 이라는 단어를 인식 못할 경우 `\"돈쭐내러\", \"돈쭐나\", \"돈쭐냄\"` 등이 다 등록 될 수 있다.\n",
    "    -   OOV(Out Of Vocab)에 대응하기 어렵게 만든다. - 같은 어근의 단어가 있지만 조사등이 바뀐 신조어등을 OOV로 인식할 수있다.\n",
    "        > -   Out Of Vocab(OOV)\n",
    "        >     -   Vocab(어휘사전): 코퍼스를 구성하는 토큰들의 모음이다.\n",
    "        >     -   OOV는 어휘사전에 없는 토큰을 말한다.\n",
    "\n",
    "# Subword Tokenization(하위 단어 토큰화)\n",
    "\n",
    "## 개념\n",
    "\n",
    "-   하나의 단어를 작은 단위의 하위단어(subword)들의 조합으로 나누는 방식으로 토큰화 한다.\n",
    "    -   자주 쓰이는 문자조합은 더 작은 subword로 분리하면 안된다.\n",
    "    -   희귀한 문자조합은 의미있는 더 작은 subword로 분리되어야 한다\n",
    "    -   ex) normally 가 희귀 단어로 분류되면 'normal'과 'ly' 로 분리한다.\n",
    "\n",
    "## 잇점\n",
    "\n",
    "-   token 단어의 길이를 줄일 수 있어 처리 속도가 빨라진다.\n",
    "-   OOV 문제, 신조어, 은어, 고유어등의 문제를 완화할 수 있다.\n",
    "\n",
    "## 방법\n",
    "\n",
    "-   Byte Pair Encoding (BPE), Word Piece, Unigram Model 등\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding\n",
    "\n",
    "-   원래 Text data 압축을 위해 만들어진 방법으로 text 에서 많이 등장하는 두글자 쌍의 조합을 찾아 부호화하는 알고리즘이다.\n",
    "-   연속된 글자 쌍이 더 나타나지 않거나 정해진 어휘사전 크기에 도달 할 때 까지 조합을 찾아 부호화 하는 작업을 반복한다.\n",
    "\n",
    "## text 압축 방식의 예\n",
    "\n",
    "-   원문: abracadabra\n",
    "\n",
    "1. AracadAra: ab -> A :=> 원문에서 가장 빈도수 많은 ab를 A(부호로 아무 글자나 사용할 수 있다.)로 치환\n",
    "2. ABcadAB: ra -> B :=> 1에서 가장 빈도수가 많은 ra를 B로 치환\n",
    "3. CcadC: AB -> C :=> 2에서 가장 빈도수 많은 AB를 C로 치환한다.(치환된 글자 쌍도 변환대상에 포함된다.)\n",
    "\n",
    "## BPE Tokenizer 방식\n",
    "\n",
    "BPE 토크나이저는 자주 등장하는 글자 쌍을 찾아 치환하는 대신 **단어 사전**에 추가한다.\n",
    "\n",
    "### 예)\n",
    "\n",
    "1. 말뭉치의 토큰들의 빈도수, 어휘사전은 아래와 같을 경우\n",
    "    - 빈도사전: ('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)\n",
    "    - 어휘사전: ['low', 'lower', 'newest', 'widest']\n",
    "2. 빈도 사전내의 모든 단어들을 글자 단위로 나눈다.\n",
    "    - 빈도사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
    "3. 빈도 사전을 기준으로 가장 자주 등장하는 글자 쌍(byte pair)를 찾는다. 위에서는 **'e'와 's'가 총 9번으로 가장 많이 등장함**. 'e'와 's'를 'es'로 합치고 어휘 사전에 추가한다.\n",
    "    - 빈도사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', **'es'**, 't', 6), ('w', 'i', 'd', **'es'**, 't', 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**]\n",
    "4. 3 번의 과정을 계속 반복한다. 빈도수가 가장 많은 'es'와 't' 쌍을 'est'로 병합하고 'est'를 어휘 사전에 추가한다.\n",
    "    - 빈도사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', **'est'**, 6), ('w', 'i', 'd', **'est'**, 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**, **'est'**]\n",
    "5. 만약 10번 반복했다고 하면 다음과 같은 빈도 사전과 어휘 사전이 생성된다.\n",
    "    - 빈도 사전: (**'low'**, 5), (**'low'**, 'e', 'r', 2), ('n', 'e', 'w', **'est'**, 6), ('w', 'i', 'd', **'est'**, 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**, **'est'**, **'lo'**,**'low'**, **'low'**, **'ne'**, **'new'**, **'newest'**, **'wi'**, **'wid'**, **'widest'**]\n",
    "\n",
    "-   위와 같이 어휘 사전이 만들어 지면 원래 어휘사전에 없던 것들에 대한 처리를 할 수있다.\n",
    "    -   ex)\n",
    "        -   'newer' :=> 'new', 'e', 'r',\n",
    "        -   'lowest' :=> 'low', 'est'\n",
    "        -   'wider' :=> 'wid', 'e', 'r'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 조회\n",
    "\n",
    "-   Korpora Dataset\n",
    "    -   2017년 8월 부터 2019년 3월 까지 청와대 청원 게시판에 올라온 텍스트 말뭉치\n",
    "    -   설치: `pip install korpora`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/petitions_archive\n",
      "    References :\n",
      "\n",
      "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
      "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
      "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
      "    단 청원의 동의 개수는 수집됩니다.\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 41.4MB/s]\n",
      "[korean_petitions] download petitions_2017-09: 20.4MB [00:00, 84.3MB/s]                                \n",
      "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 54.4MB/s]                                \n",
      "[korean_petitions] download petitions_2017-11: 28.4MB [00:01, 28.0MB/s]                                \n",
      "[korean_petitions] download petitions_2017-12: 29.0MB [00:00, 34.2MB/s]                                \n",
      "[korean_petitions] download petitions_2018-01: 43.9MB [00:02, 18.4MB/s]                                \n",
      "[korean_petitions] download petitions_2018-02: 33.8MB [00:00, 59.7MB/s]                                \n",
      "[korean_petitions] download petitions_2018-03: 34.3MB [00:01, 23.7MB/s]                                \n",
      "[korean_petitions] download petitions_2018-04: 35.5MB [00:01, 22.1MB/s]                                \n",
      "[korean_petitions] download petitions_2018-05: 37.5MB [00:01, 21.2MB/s]                                \n",
      "[korean_petitions] download petitions_2018-06: 37.8MB [00:02, 13.9MB/s]                                \n",
      "[korean_petitions] download petitions_2018-07: 40.5MB [00:02, 20.1MB/s]                                \n",
      "[korean_petitions] download petitions_2018-08: 39.8MB [00:02, 19.7MB/s]                                \n",
      "[korean_petitions] download petitions_2018-09: 36.1MB [00:01, 22.5MB/s]                                \n",
      "[korean_petitions] download petitions_2018-10: 38.1MB [00:01, 20.1MB/s]                                \n",
      "[korean_petitions] download petitions_2018-11: 37.7MB [00:02, 18.4MB/s]                                \n",
      "[korean_petitions] download petitions_2018-12: 33.0MB [00:01, 24.6MB/s]                                \n",
      "[korean_petitions] download petitions_2019-01: 34.8MB [00:01, 26.1MB/s]                                \n",
      "[korean_petitions] download petitions_2019-02: 30.8MB [00:01, 19.0MB/s]                                \n",
      "[korean_petitions] download petitions_2019-03: 34.9MB [00:02, 16.8MB/s]                                \n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "corpus = Korpora.load(\"korean_petitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Korpora.korpus_korean_petitions.KoreanPetitionsKorpus"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 433631\n"
     ]
    }
   ],
   "source": [
    "petitions = corpus.get_all_texts()\n",
    "print(type(petitions), len(petitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'청소년보호법을 악용하는 사람들이 늘고있습니다 반드시 폐지해서 더 이상 이런끔찍한일을 보지않게 해주세요..'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petitions[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일에 하나의 말뭉치로 저장.\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "path = \"data/petitions_corpus.txt\"\n",
    "with open(path, \"wt\", encoding='utf-8') as fw:\n",
    "    for txt in petitions:\n",
    "        fw.write(txt+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentencepiece\n",
    "\n",
    "-   구글에서 개발한 subword tokenizer 라이브러리로 BPE 인코딩 방식과 유사한 알고리즘을 이용해 토큰화하고 단어 사전을 생성한다.\n",
    "-   설치: `pip install sentencepiece`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 30.7/991.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 276.5/991.5 kB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 991.5/991.5 kB 10.4 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 등 학습 결과 저장할 디렉토로\n",
    "os.makedirs(\"saved_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "-   가지고 있는 말뭉치를 이용해 tokenzier를 만든다.\n",
    "-   `SentencePieceTrainer.Train()`\n",
    "-   Parameter\n",
    "    -   input: 학습데이터 경로\n",
    "    -   model_prefix: 학습된 모델 파일 저장이름\n",
    "    -   vocab_size: 어휘 사전 크기\n",
    "    -   model_type: 토크나이저 알고리즘 선택 - 'unigram', 'bpe', 'char', 'word'\n",
    "    -   max_sentence_length: 최대 문장 길이\n",
    "\n",
    "### 학습 결과파일\n",
    "\n",
    "-   model_prefix지정파일명.model : 학습된 tokenizer\n",
    "-   model_prefix지정파일명.vocab : 어휘사전이 저장된 파일 (text 파일)\n",
    "\n",
    "> token에 `_` 가 붙은 것(under score가 아님. LOWER ONE EIGHTH BLOCK 문자로 unicode \\u2581 이다.)\n",
    "> sentencepiece는 공백을 토큰화 하는 과정에 `_` 로 표현한다. 그래서 'hello world'는 'hello_world'로 처리해 토큰화 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_가\n",
      "▁가\n"
     ]
    }
   ],
   "source": [
    "print(\"_가\")\n",
    "print(\"\\u2581가\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceTrainer      # 어휘사전을 만드는 클래스\n",
    "from sentencepiece import SentencePieceProcessor  # Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린시간: 108.50059247016907초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "SentencePieceTrainer.Train(\n",
    "    \"--input=data/petitions_corpus.txt \\\n",
    "    --model_prefix=saved_models/petitions_bpe \\\n",
    "    --vocab_size=10000 \\\n",
    "    --model_type=bpe \\\n",
    "    --unk_id=0\"\n",
    ")\n",
    "e = time.time()\n",
    "print(f'걸린시간: {e-s}초')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "-   `SentencePieceProcessor`\n",
    "    -   SentencePieceTrainer.Train()으로 학습한 모델을 이용해 Tokenizer생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SentencePieceProcessor()\n",
    "# 저장된 학습 모델 파일을 로딩\n",
    "tokenizer.load(\"saved_models/petitions_bpe.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁안녕하세요', '.', '▁현재', '▁사', '대', ',', '▁교', '대', '▁등', '▁교', '원', '양', '성', '학교', '들의', '▁예비', '교사', '들이', '▁임용', '절', '벽', '에', '▁매우', '▁힘들어', '▁하고', '▁있는', '▁줄', '로', '▁압니다', '.', '▁정부', '▁부처', '에서는', '▁영양', '사의', '▁영양', \"'\", '교사', \"'\", '화', ',', '▁폭', '발', '적인', '▁영양', \"'\", '교사', \"'\", '▁채용', ',', '▁기간제', '▁교사', ',', '▁영', '전', '강', ',', '▁스', '강', '의', '▁무기계약', '직', '화가', '▁그들의', '▁임용', '▁절', '벽', '과는', '▁전혀', '▁무관', '한', '▁일이라고', '▁주장', '하고', '▁있지만', '▁조금만', '▁생각해', '보면', '▁전혀', '▁설득', '력', '▁없는', '▁말', '이라고', '▁생각합니다', '.', '▁학교', '▁수가', '▁같고', ',', '▁학생', '▁수가', '▁동일', '한데', '▁영양', '교사', '와', '▁기간제', '▁교사', ',', '▁영', '전', '강', '▁스', '강', '이', '▁학교에', '▁늘어나', '게', '▁되면', '▁당연히', '▁정', '규', '▁교', '원의', '▁수는', '▁줄어들', '게', '▁되지', '▁않겠습니까', '?', '▁기간제', '▁교사', ',', '▁영', '전', '강', ',', '▁스', '강', '의', '▁무기계약', '직', '화', ',', '▁정규직', '화', '▁꼭', '▁전면', '▁백', '지', '화', '해주십시오', '.', '▁백', '년대', '계', '인', '▁국가의', '▁교육', '에', '▁달', '린', '▁문제입니다', '.', '▁단순히', '▁대통령님의', '▁일자리', '▁공약', ',', '▁8', '1', '만', '개', '▁일자리', '▁창출', '▁공약', '을', '▁지키', '시고', '자', '▁돌', '이', '킬', '▁수', '▁없는', '▁실', '수는', '▁하지', '▁않으', '시길', '▁바랍니다', '.', '▁세계', '▁어느', '▁나라', '와', '▁비교', '해도', ',', '▁한국', '▁교', '원의', '▁수준', '과', '▁질', '은', '▁최고', '▁수준입니다', '.', '▁고', '등', '교육을', '▁받고', '▁어려운', '▁국가', '▁고', '시를', '▁통과', '해야만', '▁대한민국', '▁공', '립', '▁학교의', '▁교', '단에', '▁설', '▁수', '▁있고', ',', '▁이러한', '▁과', '정이', '▁힘들', '기는', '▁하지만', '▁교', '원', '들이', '▁교육', '자로서', '의', '▁사', '명', '감과', '▁자부', '심을', '▁갖고', '▁교육', '하게', '▁되는', '▁원', '동', '력이', '기도', '▁합니다', '.', '▁자격', '도', '▁없는', '▁비정', '규', '▁인력', '들을', '▁일자리', '▁늘', '리기', '▁명목', '▁하', '에', '▁학교', '로', '▁들이', '게', '▁되면', ',', '▁그들이', '▁무슨', '▁', '낯', '으로', '▁대한민국이', \"▁'\", '공', '정한', '▁사회', \"'\", '▁라고', '▁아이들에게', '▁가르', '칠', '▁수', '▁있겠습니까', '?', '▁그들이', '▁가르', '치는', '▁것을', '▁학부모', '와', '▁학생들이', '▁납득', '할', '▁수', '▁있겠', '으며', ',', '▁학생들은', '▁공부를', '▁열심히', '▁해야하는', '▁이유를', '▁찾을', '▁수', '나', '▁있겠습니까', '?', '▁열심히', '▁안', '▁해도', '▁떼', '▁쓰', '면', '▁되는', '▁세상', '이라고', '▁생각하지', '▁않겠습니까', '?', '▁영양', '사의', '▁영양', '교사', '화', '도', '▁재', '고', '해주십시오', '.', '▁영양', '사', '분들', '▁정말', '▁너무나', '▁고', '마', '운', '▁분들', '입니다', '.', '▁학생들의', '▁건강', '과', '▁영양', '?', '▁당연히', '▁성장', '기에', '▁있는', '▁아이들에게', '▁필수', '적이고', '▁중요한', '▁문제입니다', '.', '▁하지만', '▁이들이', '▁왜', '▁교사', '입니까', '.', '▁유', '래', '를', '▁찾아', '▁볼', '▁수', '▁없는', '▁영양', '사의', \"▁'\", '교사', \"'\", '화', '.', '▁정말', '▁대통령님이', '▁생각', '하신', '▁아이', '디어', '라고', '▁믿', '기', '▁싫', '을', '▁정도로', '▁납득', '하기', '▁어렵습니다', '.', '▁중', '등은', '▁실', '과', '교', '과', '▁교사가', '▁존재', '하지', '요', '?', '▁초등', '▁역시', '▁임용', '▁시험', '에', '▁실', '과', '가', '▁포함', '돼', '▁있으며', '▁학교', '▁현장', '에서도', '▁정', '규', '▁교', '원이', '▁직접', '▁실', '과', '▁과', '목을', '▁학생들에게', '▁가르', '칩니다', '.', '▁영양', \"'\", '교사', \"'\", ',', '▁아니', '▁영양', '사가', '▁학생들에게', '▁실', '과', '를', '▁가르', '치지', '▁않습니다', '.', '▁아니', '▁그', '▁어떤', '▁것도', '▁가르', '치지', '▁않습니다', '.', '▁올해', '▁대통령님', '▁취임', '▁후에', '▁초등', ',', '▁중', '등', '▁임용', '▁티', '오', '가', '▁초', '전', '박', '살', '▁나는', '▁동안', '▁영양', \"'\", '교사', \"'\", '▁티', '오는', '▁폭', '발', '적으로', '▁확대', '된', '▁줄', '로', '▁압니다', '.', '▁학생들의', '▁교육을', '▁위해', '▁정말', '▁교', '원의', '▁수를', '▁줄이고', ',', '▁영양', '▁교사의', '▁수를', '▁늘리는', '▁것이', '▁올바른', '▁해', '답', '인지', '▁묻고', '▁싶습니다', '.', '▁마지막으로', '▁교', '원', '▁당', '▁학생', '▁수', '.', '▁이', '▁통계', '도', '▁제대로', '▁내', '주시기', '▁바랍니다', '.', '▁다른', '▁나라', '들은', \"▁'\", '정', '규', '▁교', '원', \"'\", ',', '▁즉', '▁담', '임', '이나', '▁교과', '▁교사', '들로', '만', '▁통', '계를', '▁내', '는데', '(', '너무', '나', '▁당연한', '▁것이', '지요', ')', '▁왜', '▁한국은', '▁보건', ',', '▁영양', ',', '▁기간제', ',', '▁영', '전', '강', ',', '▁스', '강', '▁까지', '▁다', '▁포함', '해서', '▁교', '원', '▁수', '▁통', '계를', '▁내는', '건가요', '?', '▁이런', '▁통', '계의', '▁장난', '을', '▁통해', '▁OECD', '▁평균', '▁교', '원', '▁당', '▁학생', '▁수', '와', '▁거의', '▁비슷한', '▁수준', '에', '▁이르', '렀', '다고', '▁주장', '하시는', '건가요', '?', '▁학교는', '▁교육의', '▁장', '이고', '▁학생들의', '▁공간', '이지', ',', '▁인력', '▁센터', '가', '▁아닙니다', '.', '▁부탁드립니다', '.', '▁부디', '▁넓', '은', '▁안', '목으로', '▁멀리', '▁내', '다', '봐', '주시길', '▁간곡히', '▁부탁드립니다', '.']\n"
     ]
    }
   ],
   "source": [
    "txt = petitions[0]\n",
    "print(tokenizer.EncodeAsPieces(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁이', '정', '후', '는', '▁22', '일', '(', '한국', '시간', ')', '▁미국', '▁', '캘', '리', '포', '니', '아주', '▁', '샌', '프', '란', '시', '스', '코', '의', '▁오', '라', '클', '▁파', '크', '에서', '▁열', '린', '▁애', '리', '조', '나', '▁다', '이', '아', '몬', '드', '백', '스', '와', '▁20', '24', '▁메', '이', '저', '리', '그', '(', 'M', 'L', 'B', ')', '▁홈', '경', '기에', '▁1', '번', '▁타', '자', '▁겸', '▁중', '견', '수로', '▁선발', '▁출전', ',', '▁4', '타', '석', '▁2', '타', '수', '▁무', '안', '타', '▁2', '사', '사', '구를', '▁기록', '했다', '.', '▁시', '즌', '▁타', '율', '은', '▁0', '.2', '8', '9', '에서', '▁0', '.2', '8', '2', '로', '▁떨어', '졌다', '.']\n"
     ]
    }
   ],
   "source": [
    "txt2 = \"이정후는 22일(한국시간) 미국 캘리포니아주 샌프란시스코의 오라클 파크에서 열린 애리조나 다이아몬드백스와 2024 메이저리그(MLB) 홈경기에 1번 타자 겸 중견수로 선발 출전, 4타석 2타수 무안타 2사사구를 기록했다. 시즌 타율은 0.289에서 0.282로 떨어졌다.\"\n",
    "print(tokenizer.EncodeAsPieces(txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[667, 8515, 248, 9, 8533, 8553, 121, 8533, 138, 121, 8557, 8724, 8592, 413, 104, 2416, 2001, 39, 6015, 8774, 9148, 8522, 1187, 1576, 168, 55, 310, 8527, 3271, 8515, 132, 4457, 355, 7973, 967, 7973, 8763, 2001, 8763, 8630, 8553, 352, 8651, 164, 7973, 8763, 2001, 8763, 1320, 8553, 7318, 1337, 8553, 234, 8572, 8682, 8553, 356, 8682, 8526, 7584, 8643, 1460, 2180, 6015, 409, 9148, 2989, 1034, 5341, 8525, 8048, 1429, 22, 1142, 4933, 1580, 1219, 1034, 7490, 8677, 228, 68, 331, 196, 8515, 389, 1625, 8445, 8553, 328, 1625, 1961, 2147, 7973, 2001, 8628, 7318, 1337, 8553, 234, 8572, 8682, 356, 8682, 8513, 3734, 2242, 8552, 1378, 1500, 15, 8802, 121, 1310, 3751, 3451, 8552, 932, 4866, 8581, 7318, 1337, 8553, 234, 8572, 8682, 8553, 356, 8682, 8526, 7584, 8643, 8630, 8553, 1764, 8630, 533, 3870, 1296, 8520, 8630, 3288, 8515, 1296, 4939, 8646, 8538, 1781, 335, 8522, 497, 8809, 3728, 8515, 4084, 4119, 740, 2556, 8553, 570, 8582, 8555, 8629, 740, 2528, 2556, 8521, 1272, 415, 8540, 338, 8513, 9167, 19, 228, 137, 2043, 325, 3223, 4410, 307, 8515, 919, 766, 153, 8628, 3075, 596, 8553, 255, 121, 1310, 1150, 8578, 803, 8532, 1987, 8041, 8515, 75, 8632, 4159, 649, 1408, 178, 75, 2698, 2321, 6147, 156, 47, 8729, 8327, 121, 6075, 411, 19, 562, 8553, 702, 212, 753, 760, 623, 280, 121, 8557, 39, 335, 5293, 8526, 9, 8647, 7215, 7067, 1734, 2487, 335, 96, 467, 195, 8593, 826, 554, 88, 8515, 1075, 8524, 228, 1778, 8802, 3194, 143, 740, 453, 2976, 7552, 7, 8522, 389, 8527, 3155, 8552, 1378, 8553, 2501, 704, 8512, 9650, 11, 2136, 347, 8586, 930, 250, 8763, 1768, 3842, 2748, 9134, 19, 5055, 8581, 2501, 2748, 1080, 510, 2402, 8628, 2165, 3418, 8596, 19, 6904, 472, 8553, 3413, 3957, 950, 5554, 3764, 7499, 19, 8544, 5055, 8581, 950, 37, 952, 3695, 354, 8558, 467, 1344, 331, 4559, 4866, 8581, 7973, 967, 7973, 2001, 8630, 8524, 179, 8519, 3288, 8515, 7973, 8529, 1316, 232, 1003, 75, 8612, 8675, 2117, 23, 8515, 3337, 807, 8578, 7973, 8581, 1500, 2515, 405, 55, 3842, 4028, 2006, 1494, 3728, 8515, 280, 3979, 185, 1337, 543, 8515, 119, 8693, 8536, 1216, 780, 19, 228, 7973, 967, 347, 2001, 8763, 8630, 8515, 232, 4363, 52, 1653, 175, 4835, 134, 792, 8534, 1427, 8521, 2300, 3418, 289, 4005, 8515, 84, 6437, 137, 8578, 8624, 8578, 7323, 1246, 158, 8546, 8581, 4452, 1903, 6015, 1000, 8522, 137, 8578, 8523, 1238, 9055, 1482, 389, 2439, 747, 15, 8802, 121, 645, 923, 137, 8578, 212, 4086, 5157, 2748, 5609, 8515, 7973, 8763, 2001, 8763, 8553, 117, 7973, 483, 5157, 137, 8578, 8536, 2748, 3873, 571, 8515, 117, 8, 636, 1157, 2748, 3873, 571, 8515, 1740, 329, 8350, 4195, 4452, 8553, 84, 8632, 6015, 3233, 8637, 8523, 414, 8572, 8758, 8668, 2099, 1714, 7973, 8763, 2001, 8763, 3233, 1574, 352, 8651, 140, 2462, 8679, 310, 8527, 3271, 8515, 3337, 2399, 260, 232, 121, 1310, 8165, 4288, 8553, 7973, 8497, 8165, 8065, 202, 2965, 28, 8817, 523, 4014, 1061, 8515, 4895, 121, 8557, 115, 328, 19, 8515, 4, 6996, 8524, 518, 71, 2901, 307, 8515, 276, 153, 80, 347, 8541, 8802, 121, 8557, 8763, 8553, 918, 428, 8666, 200, 5130, 1337, 2304, 8555, 180, 1476, 71, 85, 8702, 3057, 8544, 3375, 202, 410, 8696, 185, 3934, 2318, 8553, 7973, 8553, 7318, 8553, 234, 8572, 8682, 8553, 356, 8682, 1538, 38, 1238, 91, 121, 8557, 19, 180, 1476, 2628, 1413, 8581, 150, 180, 5687, 2408, 8521, 820, 6921, 2721, 121, 8557, 115, 328, 19, 8628, 1193, 4318, 1150, 8522, 6074, 9344, 45, 1429, 1165, 1413, 8581, 6556, 7331, 177, 239, 3337, 3338, 821, 8553, 3194, 6618, 8523, 592, 8515, 508, 8515, 1321, 4182, 8532, 37, 3675, 7130, 71, 8514, 8931, 2547, 1958, 508, 8515]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.EncodeAsIds(txt))\n",
    "# 각 토큰에 부여된 ID(정수) 로 토큰화."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁이', '정', '후', '는', '▁22', '일', '(', '한국', '시간', ')', '▁미국', '▁', '캘', '리', '포', '니', '아주', '▁', '샌', '프', '란', '시', '스', '코', '의', '▁오', '라', '클', '▁파', '크', '에서', '▁열', '린', '▁애', '리', '조', '나', '▁다', '이', '아', '몬', '드', '백', '스', '와', '▁20', '24', '▁메', '이', '저', '리', '그', '(', 'M', 'L', 'B', ')', '▁홈', '경', '기에', '▁1', '번', '▁타', '자', '▁겸', '▁중', '견', '수로', '▁선발', '▁출전', ',', '▁4', '타', '석', '▁2', '타', '수', '▁무', '안', '타', '▁2', '사', '사', '구를', '▁기록', '했다', '.', '▁시', '즌', '▁타', '율', '은', '▁0', '.2', '8', '9', '에서', '▁0', '.2', '8', '2', '로', '▁떨어', '졌다', '.']\n",
      "이정후는 22일(한국시간) 미국 캘리포니아주 샌프란시스코의 오라클 파크에서 열린 애리조나 다이아몬드백스와 2024 메이저리그(MLB) 홈경기에 1번 타자 겸 중견수로 선발 출전, 4타석 2타수 무안타 2사사구를 기록했다. 시즌 타율은 0.289에서 0.282로 떨어졌다.\n"
     ]
    }
   ],
   "source": [
    "e1 = tokenizer.EncodeAsPieces(txt2)\n",
    "print(e1)\n",
    "# 인코딩(토큰화)된 것을 원래 문장으로 복원.\n",
    "d1 = tokenizer.DecodePieces(e1)\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8541, 8722, 8516, 4675, 8562, 8702, 1488, 266, 8696, 674, 8512, 0, 8550, 8784, 8517, 8307, 8512, 9821, 8881, 8833, 8535, 8638, 8964, 8526, 118, 8545, 9300, 425, 8880, 18, 392, 8809, 505, 8550, 8598, 8544, 38, 8513, 8537, 9603, 8665, 8938, 8638, 8628, 211, 5743, 1355, 8513, 8590, 8550, 8559, 8702, 9108, 9184, 9018, 8696, 3039, 8594, 405, 43, 8705, 552, 8540, 7665, 84, 8897, 2709, 4261, 7463, 8553, 216, 8815, 8918, 56, 8815, 8549, 61, 8576, 8815, 56, 8529, 8529, 2155, 3077, 1008, 8515, 40, 9554, 552, 8949, 8532, 1614, 2192, 8753, 8790, 18, 1614, 2192, 8753, 8602, 8527, 1120, 5101, 8515]\n",
      "이정후는 22일(한국시간) 미국  ⁇ 리포니아주 샌프란시스코의 오라클 파크에서 열린 애리조나 다이아몬드백스와 2024 메이저리그(MLB) 홈경기에 1번 타자 겸 중견수로 선발 출전, 4타석 2타수 무안타 2사사구를 기록했다. 시즌 타율은 0.289에서 0.282로 떨어졌다.\n"
     ]
    }
   ],
   "source": [
    "e2 = tokenizer.EncodeAsIds(txt2)\n",
    "print(e2)\n",
    "# 복원\n",
    "d2 = tokenizer.DecodeIds(e2)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총토큰개수: 10000\n",
      "id->piece: ▁이\n",
      "piece -> id: 5496\n"
     ]
    }
   ],
   "source": [
    "### 학습된 모델 정보\n",
    "print(\"총토큰개수:\", tokenizer.GetPieceSize())\n",
    "print(\"id->piece:\", tokenizer.IdToPiece(4))\n",
    "print(\"piece -> id:\", tokenizer.PieceToId('경기'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordpiece tokenizer\n",
    "\n",
    "-   Byte Pair Encoding 이 빈도 기반이라면 wordpiece tokenizer는 확률 기반으로 글자 쌍을 병합한다.\n",
    "-   두개 글자 쌍의 빈도수를 각 개별 글자 빈도수의 곱으로 나눈 점수가 가장 높은 순서대로 글자쌍을 묶어 나간다.\n",
    "\n",
    "$$\n",
    "score = \\cfrac{f(x, y)}{f(x)\\cdot f(y)}\n",
    "$$\n",
    "\n",
    "함수 f는 빈도를 나타내며 x, y는 병합하려는 하위 단어이다.\n",
    "\n",
    "-   빈도사전: ('l','o','w', 5), ('l','o','w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
    "-   어휘사전: ('d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w')\n",
    "-   가장 빈도수가 높은 쌍은 'e','s'로 9번 등장한다. 이때 각 글자는 전체에서 각각 'e'는 17번, 's'는 9번 등장한다. 위 공식에 대입하면 score는 $\\frac{9}{17 \\times 9} \\approx 0.06$ 이다.\n",
    "-   'i'와 'd' 쌍은 3번만 등장하지만 전체에서 각각 'i' 3번, 'd' 3번 등장한다. 그래서 score는 $\\frac{3}{3 \\times 3} \\approx 0.33$ 이다.\n",
    "-   나타난 빈도수는 'es' 가 많치만 더 높은 score를 가지는 'id' 쌍을 병합한다.\n",
    "-   빈도사전: ('l','o','w', 5), ('l','o','w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', **'id'**, 'e', 's', 't', 3)\n",
    "-   어휘사전: ('d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'id'**)\n",
    "    위의 작업을 반복해 연속된 글자 쌍이 더이상 나타나지 않거나 어휘 사전 max 크기에 도달할 때 까지 학습한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허깅페이스 Tokenizers 라이브러리 사용\n",
    "\n",
    "-   Normalization(정규화), 사전토큰화(Pre-tokenization)을 제공\n",
    "-   정규화\n",
    "    -   일관된 형식으로 텍스트를 표준화하고, 모호함 제거를 위해 일부 문자 제거 또는 대체 작업을 한다.\n",
    "-   사전 토큰화\n",
    "    -   입력문장을 토큰화하기 전에 단어와 같은 작은 단위로 나누는 기능 제공.\n",
    "-   설치: `pip install tokenizers`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\classes\\da-35\\06_nlp_preprocessing\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\classes\\da-35\\06_nlp_preprocessing\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\classes\\da-35\\06_nlp_preprocessing\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\classes\\da-35\\06_nlp_preprocessing\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\classes\\da-35\\06_nlp_preprocessing\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\classes\\da-35\\06_nlp_preprocessing\\env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\classes\\da-35\\06_nlp_preprocessing\\env\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\classes\\da-35\\06_nlp_preprocessing\\env\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\classes\\da-35\\06_nlp_preprocessing\\env\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\classes\\da-35\\06_nlp_preprocessing\\env\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 28.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 388.9/388.9 kB 25.2 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 172.0/172.0 kB ? eta 0:00:00\n",
      "Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: fsspec, filelock, huggingface-hub, tokenizers\n",
      "Successfully installed filelock-3.13.4 fsspec-2024.3.1 huggingface-hub-0.22.2 tokenizers-0.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece, BPE # Byte Pair Encoding 알고리즘.\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer, BpeTrainer\n",
    "import time\n",
    "\n",
    "path = \"data/petitions_corpus.txt\"\n",
    "# 토크나이저 알고리즘 객체를 넣어서 tokenizer 생성.\n",
    "wp_tokenizer = Tokenizer(WordPiece(unk_token='[UNK]'))  \n",
    "# Pre tokenizer 설정. (1차 토큰화 작업단위)\n",
    "wp_tokenizer.pre_tokenizer = Whitespace()\n",
    "# Trainer 생성 -> 어휘사전 어떻게 만들지 설정. \n",
    "trainer = WordPieceTrainer(vocab_size=20000, special_tokens=[\"[UNK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린시간: 49.07844352722168초\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SentencePieceProcessor' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 저장\u001b[39;00m\n\u001b[0;32m     21\u001b[0m saved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models/petitions_wordpiece.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(saved_path)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SentencePieceProcessor' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# 학습 -> 50000줄씩 끊어서 학습.\n",
    "batch_size = 50_000  # 몇 라인단위로 학습 시킬지.\n",
    "current_batch = [] # 학습시킬 문자열들을 담을 리스트\n",
    "text_path = \"data/petitions_corpus.txt\"\n",
    "with open(text_path, 'rt', encoding='utf-8') as fr:\n",
    "    s = time.time()\n",
    "    # batch_size 줄만큼 current_batch 리스트에 저장 한 뒤 학습.\n",
    "    for line in fr:\n",
    "        current_batch.append(line)\n",
    "        if len(current_batch) == batch_size: # batch size만큼 읽었다면 학습\n",
    "            # 학습\n",
    "            wp_tokenizer.train_from_iterator(current_batch, trainer)\n",
    "            # 리스트 비우기\n",
    "            current_batch.clear()\n",
    "    # current_batch의 있는 나머지 데이터 학습\n",
    "    wp_tokenizer.train_from_iterator(current_batch, trainer)\n",
    "    e = time.time()\n",
    "\n",
    "print(f\"걸린시간: {e-s}초\")\n",
    "# 저장\n",
    "saved_path = \"saved_models/petitions_wordpiece.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizer.save(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 model을 loading\n",
    "saved_tokenizer = Tokenizer.from_file(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Encoding'>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "output = wp_tokenizer.encode(txt)\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요',\n",
       " '.',\n",
       " '현재',\n",
       " '사대',\n",
       " ',',\n",
       " '교대',\n",
       " '등',\n",
       " '교원',\n",
       " '##양',\n",
       " '##성',\n",
       " '##학교',\n",
       " '##들의',\n",
       " '예비',\n",
       " '##교사',\n",
       " '##들이',\n",
       " '임용',\n",
       " '##절',\n",
       " '##벽',\n",
       " '##에',\n",
       " '매우',\n",
       " '힘들어',\n",
       " '하고',\n",
       " '있는',\n",
       " '줄',\n",
       " '##로',\n",
       " '압니다',\n",
       " '.',\n",
       " '정부',\n",
       " '부처',\n",
       " '##에서는',\n",
       " '영양',\n",
       " '##사의',\n",
       " '영양',\n",
       " \"'\",\n",
       " '교사',\n",
       " \"'\",\n",
       " '화',\n",
       " ',',\n",
       " '폭발',\n",
       " '##적인',\n",
       " '영양',\n",
       " \"'\",\n",
       " '교사',\n",
       " \"'\",\n",
       " '채용',\n",
       " ',',\n",
       " '기간제',\n",
       " '교사',\n",
       " ',',\n",
       " '영',\n",
       " '##전',\n",
       " '##강',\n",
       " ',',\n",
       " '스',\n",
       " '##강',\n",
       " '##의',\n",
       " '무기계약직',\n",
       " '##화가',\n",
       " '그들의',\n",
       " '임용',\n",
       " '절',\n",
       " '##벽',\n",
       " '##과는',\n",
       " '전혀',\n",
       " '무관',\n",
       " '##한',\n",
       " '일이라고',\n",
       " '주장하고',\n",
       " '있지만',\n",
       " '조금만',\n",
       " '생각해보',\n",
       " '##면',\n",
       " '전혀',\n",
       " '설득',\n",
       " '##력',\n",
       " '없는',\n",
       " '말이',\n",
       " '##라고',\n",
       " '생각합니다',\n",
       " '.',\n",
       " '학교',\n",
       " '수가',\n",
       " '같고',\n",
       " ',',\n",
       " '학생',\n",
       " '수가',\n",
       " '동일한',\n",
       " '##데',\n",
       " '영양',\n",
       " '##교사',\n",
       " '##와',\n",
       " '기간제',\n",
       " '교사',\n",
       " ',',\n",
       " '영',\n",
       " '##전',\n",
       " '##강',\n",
       " '스',\n",
       " '##강',\n",
       " '##이',\n",
       " '학교에',\n",
       " '늘어나',\n",
       " '##게',\n",
       " '되면',\n",
       " '당연히',\n",
       " '정',\n",
       " '##규',\n",
       " '교원',\n",
       " '##의',\n",
       " '수는',\n",
       " '줄어들',\n",
       " '##게',\n",
       " '되지',\n",
       " '않겠습니까',\n",
       " '?',\n",
       " '기간제',\n",
       " '교사',\n",
       " ',',\n",
       " '영',\n",
       " '##전',\n",
       " '##강',\n",
       " ',',\n",
       " '스',\n",
       " '##강',\n",
       " '##의',\n",
       " '무기계약직',\n",
       " '##화',\n",
       " ',',\n",
       " '정규직',\n",
       " '##화',\n",
       " '꼭',\n",
       " '전면',\n",
       " '백',\n",
       " '##지',\n",
       " '##화',\n",
       " '##해주십시오',\n",
       " '.',\n",
       " '백',\n",
       " '##년',\n",
       " '##대',\n",
       " '##계',\n",
       " '##인',\n",
       " '국가의',\n",
       " '교육',\n",
       " '##에',\n",
       " '달린',\n",
       " '문제입니다',\n",
       " '.',\n",
       " '단순히',\n",
       " '대통령님의',\n",
       " '일자리',\n",
       " '공약',\n",
       " ',',\n",
       " '8',\n",
       " '##1',\n",
       " '##만',\n",
       " '##개',\n",
       " '일자리',\n",
       " '창출',\n",
       " '공약을',\n",
       " '지키',\n",
       " '##시고',\n",
       " '##자',\n",
       " '돌이',\n",
       " '##킬',\n",
       " '수',\n",
       " '없는',\n",
       " '실수',\n",
       " '##는',\n",
       " '하지',\n",
       " '않으',\n",
       " '##시길',\n",
       " '바랍니다',\n",
       " '.',\n",
       " '세계',\n",
       " '어느',\n",
       " '나라와',\n",
       " '비교',\n",
       " '##해도',\n",
       " ',',\n",
       " '한국',\n",
       " '교원',\n",
       " '##의',\n",
       " '수준',\n",
       " '##과',\n",
       " '질',\n",
       " '##은',\n",
       " '최고',\n",
       " '수준입니다',\n",
       " '.',\n",
       " '고등',\n",
       " '##교육을',\n",
       " '받고',\n",
       " '어려운',\n",
       " '국가',\n",
       " '고',\n",
       " '##시를',\n",
       " '통과',\n",
       " '##해야만',\n",
       " '대한민국',\n",
       " '공립',\n",
       " '학교의',\n",
       " '교',\n",
       " '##단에',\n",
       " '설',\n",
       " '수',\n",
       " '있고',\n",
       " ',',\n",
       " '이러한',\n",
       " '과정이',\n",
       " '힘들',\n",
       " '##기는',\n",
       " '하지만',\n",
       " '교원',\n",
       " '##들이',\n",
       " '교육',\n",
       " '##자로서',\n",
       " '##의',\n",
       " '사',\n",
       " '##명',\n",
       " '##감과',\n",
       " '자부',\n",
       " '##심을',\n",
       " '갖고',\n",
       " '교육',\n",
       " '##하게',\n",
       " '되는',\n",
       " '원',\n",
       " '##동',\n",
       " '##력이',\n",
       " '##기도',\n",
       " '합니다',\n",
       " '.',\n",
       " '자격',\n",
       " '##도',\n",
       " '없는',\n",
       " '비정',\n",
       " '##규',\n",
       " '인력',\n",
       " '##들을',\n",
       " '일자리',\n",
       " '늘리',\n",
       " '##기',\n",
       " '명목',\n",
       " '하에',\n",
       " '학교',\n",
       " '##로',\n",
       " '들이',\n",
       " '##게',\n",
       " '되면',\n",
       " ',',\n",
       " '그들이',\n",
       " '무슨',\n",
       " '낯',\n",
       " '##으로',\n",
       " '대한민국이',\n",
       " \"'\",\n",
       " '공정한',\n",
       " '사회',\n",
       " \"'\",\n",
       " '라고',\n",
       " '아이들에게',\n",
       " '가르',\n",
       " '##칠',\n",
       " '수',\n",
       " '있겠습니까',\n",
       " '?',\n",
       " '그들이',\n",
       " '가르',\n",
       " '##치는',\n",
       " '것을',\n",
       " '학부모',\n",
       " '##와',\n",
       " '학생들이',\n",
       " '납득할',\n",
       " '수',\n",
       " '있겠',\n",
       " '##으며',\n",
       " ',',\n",
       " '학생들은',\n",
       " '공부를',\n",
       " '열심히',\n",
       " '해야하는',\n",
       " '이유를',\n",
       " '찾을',\n",
       " '수',\n",
       " '##나',\n",
       " '있겠습니까',\n",
       " '?',\n",
       " '열심히',\n",
       " '안',\n",
       " '해도',\n",
       " '떼',\n",
       " '쓰면',\n",
       " '되는',\n",
       " '세상이',\n",
       " '##라고',\n",
       " '생각하지',\n",
       " '않겠습니까',\n",
       " '?',\n",
       " '영양',\n",
       " '##사의',\n",
       " '영양',\n",
       " '##교사',\n",
       " '##화',\n",
       " '##도',\n",
       " '재고',\n",
       " '##해주십시오',\n",
       " '.',\n",
       " '영양',\n",
       " '##사',\n",
       " '##분들',\n",
       " '정말',\n",
       " '너무나',\n",
       " '고마',\n",
       " '##운',\n",
       " '분들',\n",
       " '##입니다',\n",
       " '.',\n",
       " '학생들의',\n",
       " '건강과',\n",
       " '영양',\n",
       " '?',\n",
       " '당연히',\n",
       " '성장',\n",
       " '##기에',\n",
       " '있는',\n",
       " '아이들에게',\n",
       " '필수',\n",
       " '##적이고',\n",
       " '중요한',\n",
       " '문제입니다',\n",
       " '.',\n",
       " '하지만',\n",
       " '이들이',\n",
       " '왜',\n",
       " '교사',\n",
       " '##입니까',\n",
       " '.',\n",
       " '유',\n",
       " '##래',\n",
       " '##를',\n",
       " '찾아',\n",
       " '볼',\n",
       " '수',\n",
       " '없는',\n",
       " '영양',\n",
       " '##사의',\n",
       " \"'\",\n",
       " '교사',\n",
       " \"'\",\n",
       " '화',\n",
       " '.',\n",
       " '정말',\n",
       " '대통령님이',\n",
       " '생각',\n",
       " '##하신',\n",
       " '아이디어',\n",
       " '##라고',\n",
       " '믿',\n",
       " '##기',\n",
       " '싫',\n",
       " '##을',\n",
       " '정도로',\n",
       " '납득',\n",
       " '##하기',\n",
       " '어렵습니다',\n",
       " '.',\n",
       " '중',\n",
       " '##등은',\n",
       " '실',\n",
       " '##과',\n",
       " '##교',\n",
       " '##과',\n",
       " '교사가',\n",
       " '존재하지',\n",
       " '##요',\n",
       " '?',\n",
       " '초등',\n",
       " '역시',\n",
       " '임용',\n",
       " '시험',\n",
       " '##에',\n",
       " '실',\n",
       " '##과',\n",
       " '##가',\n",
       " '포함',\n",
       " '##돼',\n",
       " '있으며',\n",
       " '학교',\n",
       " '현장에서',\n",
       " '##도',\n",
       " '정',\n",
       " '##규',\n",
       " '교원',\n",
       " '##이',\n",
       " '직접',\n",
       " '실',\n",
       " '##과',\n",
       " '과목',\n",
       " '##을',\n",
       " '학생들에게',\n",
       " '가르',\n",
       " '##칩니다',\n",
       " '.',\n",
       " '영양',\n",
       " \"'\",\n",
       " '교사',\n",
       " \"',\",\n",
       " '아니',\n",
       " '영양',\n",
       " '##사가',\n",
       " '학생들에게',\n",
       " '실',\n",
       " '##과',\n",
       " '##를',\n",
       " '가르',\n",
       " '##치지',\n",
       " '않습니다',\n",
       " '.',\n",
       " '아니',\n",
       " '그',\n",
       " '어떤',\n",
       " '것도',\n",
       " '가르',\n",
       " '##치지',\n",
       " '않습니다',\n",
       " '.',\n",
       " '올해',\n",
       " '대통령님',\n",
       " '취임',\n",
       " '후에',\n",
       " '초등',\n",
       " ',',\n",
       " '중',\n",
       " '##등',\n",
       " '임용',\n",
       " '티',\n",
       " '##오',\n",
       " '##가',\n",
       " '초',\n",
       " '##전',\n",
       " '##박',\n",
       " '##살',\n",
       " '나는',\n",
       " '동안',\n",
       " '영양',\n",
       " \"'\",\n",
       " '교사',\n",
       " \"'\",\n",
       " '티',\n",
       " '##오는',\n",
       " '폭발',\n",
       " '##적으로',\n",
       " '확대',\n",
       " '##된',\n",
       " '줄',\n",
       " '##로',\n",
       " '압니다',\n",
       " '.',\n",
       " '학생들의',\n",
       " '교육을',\n",
       " '위해',\n",
       " '정말',\n",
       " '교원',\n",
       " '##의',\n",
       " '수를',\n",
       " '줄이고',\n",
       " ',',\n",
       " '영양',\n",
       " '교사의',\n",
       " '수를',\n",
       " '늘리',\n",
       " '##는',\n",
       " '것이',\n",
       " '올바른',\n",
       " '해',\n",
       " '##답',\n",
       " '##인지',\n",
       " '묻고',\n",
       " '싶습니다',\n",
       " '.',\n",
       " '마지막으로',\n",
       " '교원',\n",
       " '당',\n",
       " '학생',\n",
       " '수',\n",
       " '.',\n",
       " '이',\n",
       " '통계',\n",
       " '##도',\n",
       " '제대로',\n",
       " '내',\n",
       " '##주시기',\n",
       " '바랍니다',\n",
       " '.',\n",
       " '다른',\n",
       " '나라',\n",
       " '##들은',\n",
       " \"'\",\n",
       " '정',\n",
       " '##규',\n",
       " '교원',\n",
       " \"',\",\n",
       " '즉',\n",
       " '담임',\n",
       " '##이나',\n",
       " '교과',\n",
       " '교사',\n",
       " '##들로',\n",
       " '##만',\n",
       " '통계',\n",
       " '##를',\n",
       " '내는',\n",
       " '##데',\n",
       " '(',\n",
       " '너무나',\n",
       " '당연한',\n",
       " '것이지',\n",
       " '##요',\n",
       " ')',\n",
       " '왜',\n",
       " '한국은',\n",
       " '보건',\n",
       " ',',\n",
       " '영양',\n",
       " ',',\n",
       " '기간제',\n",
       " ',',\n",
       " '영',\n",
       " '##전',\n",
       " '##강',\n",
       " ',',\n",
       " '스',\n",
       " '##강',\n",
       " '까지',\n",
       " '다',\n",
       " '포함',\n",
       " '##해서',\n",
       " '교원',\n",
       " '수',\n",
       " '통계',\n",
       " '##를',\n",
       " '내는',\n",
       " '##건가요',\n",
       " '?',\n",
       " '이런',\n",
       " '통계',\n",
       " '##의',\n",
       " '장난',\n",
       " '##을',\n",
       " '통해',\n",
       " 'OECD',\n",
       " '평균',\n",
       " '교원',\n",
       " '당',\n",
       " '학생',\n",
       " '수',\n",
       " '##와',\n",
       " '거의',\n",
       " '비슷한',\n",
       " '수준',\n",
       " '##에',\n",
       " '이르',\n",
       " '##렀',\n",
       " '##다고',\n",
       " '주장',\n",
       " '##하시는',\n",
       " '##건가요',\n",
       " '?',\n",
       " '학교는',\n",
       " '교육의',\n",
       " '장',\n",
       " '##이고',\n",
       " '학생들의',\n",
       " '공간이',\n",
       " '##지',\n",
       " ',',\n",
       " '인력',\n",
       " '센터',\n",
       " '##가',\n",
       " '아닙니다',\n",
       " '.',\n",
       " '부탁드립니다',\n",
       " '.',\n",
       " '부디',\n",
       " '넓은',\n",
       " '안',\n",
       " '##목',\n",
       " '##으로',\n",
       " '멀리',\n",
       " '내',\n",
       " '##다',\n",
       " '##봐',\n",
       " '##주시길',\n",
       " '간곡히',\n",
       " '부탁드립니다',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰들\n",
    "output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7628,\n",
       " 13,\n",
       " 7228,\n",
       " 14909,\n",
       " 11,\n",
       " 19443,\n",
       " 1994,\n",
       " 18442,\n",
       " 3941,\n",
       " 3978,\n",
       " 7347,\n",
       " 7153,\n",
       " 9408,\n",
       " 10015,\n",
       " 7121,\n",
       " 14728,\n",
       " 4040,\n",
       " 4129,\n",
       " 3966,\n",
       " 7889,\n",
       " 9002,\n",
       " 7165,\n",
       " 7125,\n",
       " 3147,\n",
       " 4128,\n",
       " 10699,\n",
       " 13,\n",
       " 7163,\n",
       " 11349,\n",
       " 7294,\n",
       " 11202,\n",
       " 7609,\n",
       " 11202,\n",
       " 6,\n",
       " 8983,\n",
       " 6,\n",
       " 3745,\n",
       " 11,\n",
       " 16052,\n",
       " 7173,\n",
       " 11202,\n",
       " 6,\n",
       " 8983,\n",
       " 6,\n",
       " 8364,\n",
       " 11,\n",
       " 15682,\n",
       " 8983,\n",
       " 11,\n",
       " 2909,\n",
       " 4028,\n",
       " 4103,\n",
       " 11,\n",
       " 2702,\n",
       " 4103,\n",
       " 4023,\n",
       " 18028,\n",
       " 8387,\n",
       " 9058,\n",
       " 14728,\n",
       " 3088,\n",
       " 4129,\n",
       " 9728,\n",
       " 7655,\n",
       " 12002,\n",
       " 4049,\n",
       " 17438,\n",
       " 17920,\n",
       " 8061,\n",
       " 11298,\n",
       " 14836,\n",
       " 3955,\n",
       " 7655,\n",
       " 10620,\n",
       " 4075,\n",
       " 7206,\n",
       " 7517,\n",
       " 7154,\n",
       " 7255,\n",
       " 13,\n",
       " 7306,\n",
       " 8176,\n",
       " 13705,\n",
       " 11,\n",
       " 7282,\n",
       " 8176,\n",
       " 11671,\n",
       " 4214,\n",
       " 11202,\n",
       " 10015,\n",
       " 4097,\n",
       " 15682,\n",
       " 8983,\n",
       " 11,\n",
       " 2909,\n",
       " 4028,\n",
       " 4103,\n",
       " 2702,\n",
       " 4103,\n",
       " 3938,\n",
       " 10149,\n",
       " 9647,\n",
       " 3970,\n",
       " 8201,\n",
       " 8238,\n",
       " 3096,\n",
       " 4382,\n",
       " 18442,\n",
       " 4023,\n",
       " 10242,\n",
       " 10762,\n",
       " 3970,\n",
       " 7802,\n",
       " 11613,\n",
       " 30,\n",
       " 15682,\n",
       " 8983,\n",
       " 11,\n",
       " 2909,\n",
       " 4028,\n",
       " 4103,\n",
       " 11,\n",
       " 2702,\n",
       " 4103,\n",
       " 4023,\n",
       " 18028,\n",
       " 4036,\n",
       " 11,\n",
       " 8541,\n",
       " 4036,\n",
       " 1658,\n",
       " 10188,\n",
       " 2392,\n",
       " 3997,\n",
       " 4036,\n",
       " 11341,\n",
       " 13,\n",
       " 2392,\n",
       " 4179,\n",
       " 4147,\n",
       " 4154,\n",
       " 3974,\n",
       " 8770,\n",
       " 7308,\n",
       " 3966,\n",
       " 17769,\n",
       " 10894,\n",
       " 13,\n",
       " 11382,\n",
       " 12375,\n",
       " 8026,\n",
       " 9403,\n",
       " 11,\n",
       " 23,\n",
       " 4002,\n",
       " 4041,\n",
       " 4011,\n",
       " 8026,\n",
       " 10934,\n",
       " 19556,\n",
       " 8049,\n",
       " 7404,\n",
       " 4138,\n",
       " 17250,\n",
       " 4464,\n",
       " 2669,\n",
       " 7206,\n",
       " 12727,\n",
       " 3951,\n",
       " 7245,\n",
       " 9475,\n",
       " 14512,\n",
       " 7338,\n",
       " 13,\n",
       " 7573,\n",
       " 7564,\n",
       " 19391,\n",
       " 9659,\n",
       " 7486,\n",
       " 11,\n",
       " 7196,\n",
       " 18442,\n",
       " 4023,\n",
       " 7844,\n",
       " 4112,\n",
       " 3182,\n",
       " 4000,\n",
       " 8505,\n",
       " 16332,\n",
       " 13,\n",
       " 13931,\n",
       " 11082,\n",
       " 7521,\n",
       " 8282,\n",
       " 7184,\n",
       " 1518,\n",
       " 10731,\n",
       " 9035,\n",
       " 13488,\n",
       " 7168,\n",
       " 17682,\n",
       " 14749,\n",
       " 1548,\n",
       " 13365,\n",
       " 2621,\n",
       " 2669,\n",
       " 7423,\n",
       " 11,\n",
       " 7612,\n",
       " 19995,\n",
       " 7434,\n",
       " 7464,\n",
       " 7249,\n",
       " 18442,\n",
       " 7121,\n",
       " 7308,\n",
       " 11826,\n",
       " 4023,\n",
       " 2577,\n",
       " 4048,\n",
       " 14347,\n",
       " 15996,\n",
       " 7996,\n",
       " 9177,\n",
       " 7308,\n",
       " 7142,\n",
       " 7364,\n",
       " 2980,\n",
       " 4256,\n",
       " 7699,\n",
       " 7412,\n",
       " 7150,\n",
       " 13,\n",
       " 8254,\n",
       " 4026,\n",
       " 7206,\n",
       " 8438,\n",
       " 4382,\n",
       " 10468,\n",
       " 7172,\n",
       " 8026,\n",
       " 16849,\n",
       " 4012,\n",
       " 10561,\n",
       " 19133,\n",
       " 7306,\n",
       " 4128,\n",
       " 9822,\n",
       " 3970,\n",
       " 8201,\n",
       " 11,\n",
       " 9664,\n",
       " 7498,\n",
       " 1739,\n",
       " 7114,\n",
       " 9006,\n",
       " 6,\n",
       " 10766,\n",
       " 7233,\n",
       " 6,\n",
       " 7557,\n",
       " 11296,\n",
       " 9821,\n",
       " 4587,\n",
       " 2669,\n",
       " 12362,\n",
       " 30,\n",
       " 9664,\n",
       " 9821,\n",
       " 7942,\n",
       " 7348,\n",
       " 9625,\n",
       " 4097,\n",
       " 9271,\n",
       " 19078,\n",
       " 2669,\n",
       " 12654,\n",
       " 7275,\n",
       " 11,\n",
       " 10075,\n",
       " 11093,\n",
       " 8101,\n",
       " 13546,\n",
       " 9981,\n",
       " 14386,\n",
       " 2669,\n",
       " 3963,\n",
       " 12362,\n",
       " 30,\n",
       " 8101,\n",
       " 2823,\n",
       " 7792,\n",
       " 2046,\n",
       " 15921,\n",
       " 7364,\n",
       " 10697,\n",
       " 7154,\n",
       " 12072,\n",
       " 11613,\n",
       " 30,\n",
       " 11202,\n",
       " 7609,\n",
       " 11202,\n",
       " 10015,\n",
       " 4036,\n",
       " 4026,\n",
       " 11929,\n",
       " 11341,\n",
       " 13,\n",
       " 11202,\n",
       " 3999,\n",
       " 8166,\n",
       " 7223,\n",
       " 7927,\n",
       " 18660,\n",
       " 4255,\n",
       " 9139,\n",
       " 7118,\n",
       " 13,\n",
       " 10633,\n",
       " 16328,\n",
       " 11202,\n",
       " 30,\n",
       " 8238,\n",
       " 10224,\n",
       " 7283,\n",
       " 7125,\n",
       " 11296,\n",
       " 10846,\n",
       " 8726,\n",
       " 8167,\n",
       " 10894,\n",
       " 13,\n",
       " 7249,\n",
       " 10182,\n",
       " 2948,\n",
       " 8983,\n",
       " 7478,\n",
       " 13,\n",
       " 3001,\n",
       " 4043,\n",
       " 4069,\n",
       " 7919,\n",
       " 2444,\n",
       " 2669,\n",
       " 7206,\n",
       " 11202,\n",
       " 7609,\n",
       " 6,\n",
       " 8983,\n",
       " 6,\n",
       " 3745,\n",
       " 13,\n",
       " 7223,\n",
       " 11692,\n",
       " 7130,\n",
       " 8433,\n",
       " 17639,\n",
       " 7154,\n",
       " 2360,\n",
       " 4012,\n",
       " 2724,\n",
       " 4019,\n",
       " 8813,\n",
       " 10589,\n",
       " 7230,\n",
       " 12351,\n",
       " 13,\n",
       " 3151,\n",
       " 14661,\n",
       " 2722,\n",
       " 4112,\n",
       " 4054,\n",
       " 4112,\n",
       " 17319,\n",
       " 16331,\n",
       " 4065,\n",
       " 30,\n",
       " 14304,\n",
       " 8501,\n",
       " 14728,\n",
       " 8137,\n",
       " 3966,\n",
       " 2722,\n",
       " 4112,\n",
       " 4061,\n",
       " 7933,\n",
       " 4174,\n",
       " 8191,\n",
       " 7306,\n",
       " 12561,\n",
       " 4026,\n",
       " 3096,\n",
       " 4382,\n",
       " 18442,\n",
       " 3938,\n",
       " 7623,\n",
       " 2722,\n",
       " 4112,\n",
       " 14592,\n",
       " 4019,\n",
       " 13095,\n",
       " 9821,\n",
       " 13733,\n",
       " 13,\n",
       " 11202,\n",
       " 6,\n",
       " 8983,\n",
       " 14586,\n",
       " 7148,\n",
       " 11202,\n",
       " 7316,\n",
       " 13095,\n",
       " 2722,\n",
       " 4112,\n",
       " 4069,\n",
       " 9821,\n",
       " 10363,\n",
       " 7512,\n",
       " 13,\n",
       " 7148,\n",
       " 1587,\n",
       " 7451,\n",
       " 7922,\n",
       " 9821,\n",
       " 10363,\n",
       " 7512,\n",
       " 13,\n",
       " 8735,\n",
       " 7323,\n",
       " 14732,\n",
       " 10503,\n",
       " 14304,\n",
       " 11,\n",
       " 3151,\n",
       " 4185,\n",
       " 14728,\n",
       " 3563,\n",
       " 3995,\n",
       " 4061,\n",
       " 3329,\n",
       " 4028,\n",
       " 4329,\n",
       " 4272,\n",
       " 8397,\n",
       " 8289,\n",
       " 11202,\n",
       " 6,\n",
       " 8983,\n",
       " 6,\n",
       " 3563,\n",
       " 8146,\n",
       " 16052,\n",
       " 7156,\n",
       " 9345,\n",
       " 3961,\n",
       " 3147,\n",
       " 4128,\n",
       " 10699,\n",
       " 13,\n",
       " 10633,\n",
       " 9474,\n",
       " 7238,\n",
       " 7223,\n",
       " 18442,\n",
       " 4023,\n",
       " 16503,\n",
       " 11583,\n",
       " 11,\n",
       " 11202,\n",
       " 17862,\n",
       " 16503,\n",
       " 16849,\n",
       " 3951,\n",
       " 7214,\n",
       " 10240,\n",
       " 3683,\n",
       " 4232,\n",
       " 7376,\n",
       " 10940,\n",
       " 8003,\n",
       " 13,\n",
       " 11216,\n",
       " 18442,\n",
       " 1881,\n",
       " 7282,\n",
       " 2669,\n",
       " 13,\n",
       " 3031,\n",
       " 11376,\n",
       " 4026,\n",
       " 7390,\n",
       " 1743,\n",
       " 10463,\n",
       " 7338,\n",
       " 13,\n",
       " 7236,\n",
       " 7175,\n",
       " 7137,\n",
       " 6,\n",
       " 3096,\n",
       " 4382,\n",
       " 18442,\n",
       " 14586,\n",
       " 3168,\n",
       " 16221,\n",
       " 7194,\n",
       " 12521,\n",
       " 8983,\n",
       " 8884,\n",
       " 4041,\n",
       " 11376,\n",
       " 4069,\n",
       " 9872,\n",
       " 4214,\n",
       " 7,\n",
       " 7927,\n",
       " 10085,\n",
       " 15259,\n",
       " 4065,\n",
       " 8,\n",
       " 2948,\n",
       " 9876,\n",
       " 9047,\n",
       " 11,\n",
       " 11202,\n",
       " 11,\n",
       " 15682,\n",
       " 11,\n",
       " 2909,\n",
       " 4028,\n",
       " 4103,\n",
       " 11,\n",
       " 2702,\n",
       " 4103,\n",
       " 8197,\n",
       " 1867,\n",
       " 7933,\n",
       " 7141,\n",
       " 18442,\n",
       " 2669,\n",
       " 11376,\n",
       " 4069,\n",
       " 9872,\n",
       " 8395,\n",
       " 30,\n",
       " 7169,\n",
       " 11376,\n",
       " 4023,\n",
       " 9094,\n",
       " 4019,\n",
       " 7587,\n",
       " 11519,\n",
       " 9414,\n",
       " 18442,\n",
       " 1881,\n",
       " 7282,\n",
       " 2669,\n",
       " 4097,\n",
       " 7842,\n",
       " 11113,\n",
       " 7844,\n",
       " 3966,\n",
       " 11904,\n",
       " 4481,\n",
       " 7122,\n",
       " 7721,\n",
       " 8180,\n",
       " 8395,\n",
       " 30,\n",
       " 13336,\n",
       " 15728,\n",
       " 3065,\n",
       " 7192,\n",
       " 10633,\n",
       " 19696,\n",
       " 3997,\n",
       " 11,\n",
       " 10468,\n",
       " 12611,\n",
       " 4061,\n",
       " 7522,\n",
       " 13,\n",
       " 7555,\n",
       " 13,\n",
       " 8557,\n",
       " 18988,\n",
       " 2823,\n",
       " 4252,\n",
       " 7114,\n",
       " 14596,\n",
       " 1743,\n",
       " 3954,\n",
       " 4506,\n",
       " 10126,\n",
       " 9244,\n",
       " 7555,\n",
       " 13]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids\n",
    "# 각 토큰의 mapping id로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1491"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "# 토크나이저 생성 및 학습 모델 로딩\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load('saved_models/petitions_bpe.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_tokenizer(document):\n",
    "    # 클리닝(텍스트 정제)\n",
    "    return tokenizer.EncodeAsPieces(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(\n",
    "    tokenizer=bpe_tokenizer, # 토큰화 함수를 제공.\n",
    "    token_pattern=None  # 토큰기준 regexp 패턴. 토크나이저 함수를 지정하므로 default 패턴을 제거.\n",
    ")\n",
    "cv.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
