{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LinearRegression from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현할 것\n",
    "- 공부시간과 성적간의 관계를 모델링한다.\n",
    "    - **머신러닝 모델(모형)이란** 수집한 데이터를 기반으로 입력값(Feature)와 출력값(Target)간의 관계를 하나의 공식으로 정의한 함수이다. 그 공식을 찾는 과정을 **모델링**이라고 한다.\n",
    "    - 이 예제에서는 공부한 시간으로 점수를 예측하는 모델을 정의한다.\n",
    "    - 입력값과 출력값 간의 관계를 정의할 수있는 다양한 함수(공식)이 있다. 여기에서는 딥러닝과 관계가 있는 **Linear Regression** 을 사용해본다.\n",
    "\n",
    "# 데이터 확인\n",
    "- 입력데이터: 공부시간\n",
    "- 출력데이터: 성적\n",
    "\n",
    "|공부시간|점수|\n",
    "|-|-|\n",
    "|1|20|\n",
    "|2|40|\n",
    "|3|60|\n",
    "\n",
    "우리가 수집한 공부시간과 점수 데이터를 바탕으로 둘 간의 관계를 식으로 정의 할 수 있으면 **내가 몇시간 공부하면 점수를 얼마 받을 수 있는지 예측할 수 있게 된다.**   \n",
    "수집한 데이터를 기반으로 앞으로 예측할 수있는 모형을 만드는 것이 머신러닝 모델링이다.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습(훈련) 데이터셋 만들기\n",
    "- 모델을 학습시키기 위한 데이터셋을 구성한다.\n",
    "- 입력데이터와 출력데이터을 각각 다른 행렬로 구성한다.\n",
    "- 하나의 데이터 포인트의 입력/출력 값은 같은 index에 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링\n",
    "\n",
    "## 모델 정의\n",
    "\n",
    "- Feature와 Target간의 관계를 수식으로 정의한다.\n",
    "- 여기서는 공부시간(Feature)와 점수(Target)간의 관계를 정의하는데 **선형회귀(Linear Regression) 모델** 을 가설로 세우고 모델링을 한다.\n",
    "    - 많은 머신러닝 연구자들이 다양한 종류의 데이터간의 관계(패턴)를 예측할 수 있는 여러 알고리즘을 연구했다.\n",
    "    - 선형회귀 모델은 입력데이터와 출력데이터가 선형관계(linear)일때 좋은 성능을 나타낸다.\n",
    "  \n",
    "> ### 가설\n",
    "> - 아직은 이 식이 feature와 target의 관계를 잘 표현하는 함수인지 여부를 알 수없기 때문에 **이 식을 가설(hypothesis) 라고 한다.**\n",
    "> - 가설을 세우고 모델링을 한 뒤 검증을 해서 좋은 예측결과를 내면 그 가설을 최종 결과 모델로 결정한다. 예측결과가 좋지 않을 경우 새로운 가설로 모델링을 한다.\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀 (Linear Regression)\n",
    "- Feature들의 가중합을 이용해 Target을 추정한다.\n",
    "- Feature에 곱해지는 가중치(weight)들은 각 Feature가 Target 얼마나 영향을 주는지 영향도가 된다.\n",
    "    - 음수일 경우는 target값을 줄이고 양수일 경우는 target값을 늘린다.\n",
    "    - 가중치가 0에 가까울 수록 target에 영향을 주지 않는 feature이고 0에서 멀수록 target에 많은 영향을 준다.\n",
    "- 모델 학습과정에서 가장 적절한 Feature의 가중치를 찾아야 한다.\n",
    "      \n",
    "\n",
    "\\begin{align}\n",
    "&\\large \\hat{y} = Wx + b\\\\\n",
    "&\\small \\hat{y}: \\text{모델추정값}\\\\\n",
    "&\\small W: \\text{가중치}\\\\\n",
    "&\\small x: \\text{Feature(입력값)}\\\\\n",
    "&\\small b: \\text{bias(편향)}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [경사하강법을 이용한 최적화](reference/03_gradient_decending.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset 구성\n",
    "- Train data는 feature(input)와 target(output) 각각 2개의 행렬로 구성한다.\n",
    "- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다. 이 문제에서는 `공부시간` 1개의 변수를 가진다.\n",
    "- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 구성한다. 이 문제에서 예측할 항목은 `시험점수` 한개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_hours = [[1], [2], [3]]   # input (X)\n",
    "scores = [[20],[40],[60]]        # output (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# pytorch Tensor로 변환 (input/output)\n",
    "X_train = torch.tensor(study_hours, dtype=torch.float32)\n",
    "y_train = torch.tensor(scores, dtype=torch.float32)\n",
    "print(X_train.shape, y_train.size())\n",
    "\n",
    "## (3:개수, 1:feature shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 (weight, bias) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- 초기파라미터(weight, bias) ---------\n",
      "Weight shape: torch.Size([1, 1]), Bias shape: torch.Size([1])\n",
      "Weight: tensor([[1.5410]], requires_grad=True)\n",
      "Bias: tensor([-0.2934], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "# W * X + b ==> \n",
    "# Weight -> X feature 들에 곱해줄 가중치. X feature개수: 1 => weight: 1개\n",
    "# bias -> 모든 feature들의 값이 0일 때 y의 값:   1개\n",
    "## weight, bias가 경사하강법을 이용한 최적화 대상.  -> gradient를 구할 대상.\n",
    "#       -> requires_grad=True\n",
    "weight = torch.randn(1, 1, requires_grad=True) \n",
    "# 1 - input shape(입력feature 개수)    X    1 - output shape(출력값의 개수)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "print(\"------- 초기파라미터(weight, bias) ---------\") # 랜덤값\n",
    "print(f\"Weight shape: {weight.shape}, Bias shape: {bias.shape}\")\n",
    "print(\"Weight:\", weight)\n",
    "print(\"Bias:\", bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred =  X_train @ weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2476],\n",
       "        [2.7886],\n",
       "        [4.3296]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.],\n",
       "        [40.],\n",
       "        [60.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링\n",
    "1. 모델 정의 -> 선형회귀\n",
    "2. fitting -> 모델의 파라미터를 최적화. 경사하강법(gradient decent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "def linear_model(X):\n",
    "    return X @ weight + bias\n",
    "\n",
    "# 오차 계산 함수 (Loss Function, Cost Function)\n",
    "def loss_fn(pred, y):\n",
    "    # 모델추정값과 정답을 받아서 오차 계산.\n",
    "    ### 회귀 -> MSE (Mean Squared Error)\n",
    "    return torch.mean((pred - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2476],\n",
      "        [2.7886],\n",
      "        [4.3296]], grad_fn=<AddBackward0>)\n",
      "-----------\n",
      "tensor(1611.8477, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 모델로 추론 -> 오차 계산 \n",
    "pred = linear_model(X_train)\n",
    "loss = loss_fn(pred, y_train)\n",
    "print(pred)\n",
    "print(\"-----------\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "weight.data = weight.data - weight.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias.data = bias.data - bias.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = linear_model(X_train)\n",
    "loss2 = loss_fn(pred2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1541.3853, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1541.3853, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "1. 모델을 이용해 추정한다.\n",
    "   - pred = model(input)\n",
    "1. loss를 계산한다.\n",
    "   - loss = loss_fn(pred, target)\n",
    "1. 계산된 loss를 파라미터에 대해 미분하여 계산한 gradient 값을 각 파라미터에 저장한다.\n",
    "   - loss.backward()\n",
    "1. optimizer를 이용해 파라미터를 update한다.\n",
    "   - optimizer.step()  \n",
    "1. 파라미터의 gradient(미분값)을 0으로 초기화한다.\n",
    "   - optimizer.zero_grad()\n",
    "- 위의 단계를 반복한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# weight, bias\n",
    "weight = torch.randn(1, 1, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# model \n",
    "def linear_model(X):\n",
    "    return X @ weight + bias\n",
    "\n",
    "# loss fn(손실함수) - MSE\n",
    "def loss_fn(pred, y):\n",
    "    return torch.mean((pred - y)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5410]], requires_grad=True)\n",
      "tensor([-0.2934], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(weight)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2476],\n",
       "        [2.7886],\n",
       "        [4.3296]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000/1000] loss - 0.0004067078698426485\n",
      "[100/1000] loss - 0.00025138139608316123\n",
      "[200/1000] loss - 0.00015541019092779607\n",
      "[300/1000] loss - 9.610102279111743e-05\n",
      "[400/1000] loss - 5.944461372564547e-05\n",
      "[500/1000] loss - 3.677355925901793e-05\n",
      "[600/1000] loss - 2.2759641069569625e-05\n",
      "[700/1000] loss - 1.408370826538885e-05\n",
      "[800/1000] loss - 8.718849130673334e-06\n",
      "[900/1000] loss - 5.400575901148841e-06\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000  # 반복횟수 \n",
    "lr = 0.01\n",
    "for epoch in range(epochs):\n",
    "    # 1. 모델 추정\n",
    "    pred = linear_model(X_train)\n",
    "    # 2. 오차 계산\n",
    "    loss = loss_fn(pred, y_train)\n",
    "    # 3. parameter들의 gradient를 계산\n",
    "    loss.backward()\n",
    "    # 4. parameter update    \n",
    "    ### weight update\n",
    "    weight.data = weight.data - weight.grad * lr  # w.data: 현재값, w.grad: gradient값\n",
    "    ### bias update\n",
    "    bias.data = bias.data - bias.grad * lr\n",
    "    # 5. weight, bias 의 grad 값 초기화\n",
    "    weight.grad = None  # weight.grad.zero_()\n",
    "    bias.grad = None\n",
    "    ##### 로그 출력 (loss 값 출력) 100번 반복당 한번씩 출력\n",
    "    if epoch % 100 == 0: \n",
    "        print(f\"[{epoch:03d}/{epochs}] loss - {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 loss:  0.05032205954194069\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "print(\"최종 loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19.9979]], requires_grad=True)\n",
      "tensor([0.0048], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(weight)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.0027],\n",
       "        [40.0006],\n",
       "        [59.9985]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2 = linear_model(X_train)\n",
    "pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.],\n",
       "        [40.],\n",
       "        [60.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 파이토치 optimizer를 이용해서 파라미터 업데이트 \n",
    "##    w.data = w.data - w.grad * lr   (이 작업을 처리하는 함수) \n",
    "##    gradient 초기화.\n",
    "torch.manual_seed(0)\n",
    "# weight, bias\n",
    "weight = torch.randn(1, 1, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# model \n",
    "def linear_model(X):\n",
    "    return X @ weight + bias\n",
    "\n",
    "# loss fn(손실함수) - MSE\n",
    "def loss_fn(pred, y):\n",
    "    return torch.mean((pred - y)**2)\n",
    "\n",
    "# optimizer (instance) - 최적화대상 변수, learning rate\n",
    "optim = torch.optim.SGD(\n",
    "    [weight, bias], #  최적화 대상(requires_grad=True)\n",
    "    lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000/1000] - loss: 1611.84765625\n",
      "[100/1000] - loss: 3.812145471572876\n",
      "[200/1000] - loss: 2.3556692600250244\n",
      "[300/1000] - loss: 1.4556589126586914\n",
      "[400/1000] - loss: 0.8995108008384705\n",
      "[500/1000] - loss: 0.5558425784111023\n",
      "[600/1000] - loss: 0.34347668290138245\n",
      "[700/1000] - loss: 0.21224604547023773\n",
      "[800/1000] - loss: 0.13115547597408295\n",
      "[900/1000] - loss: 0.08104630559682846\n",
      "[999/1000] - loss: 0.05032205954194069\n"
     ]
    }
   ],
   "source": [
    "#### 학습\n",
    "for epoch in range(epochs):\n",
    "    # 1. 모델 추정\n",
    "    pred = linear_model(X_train)\n",
    "    # 2. loss 계산\n",
    "    loss = loss_fn(pred, y_train)\n",
    "    # 3. gradient 계산 - 역전파(backpropagation)\n",
    "    loss.backward()\n",
    "    # 4. 파라미터 최적화 -> optimizer.step()\n",
    "    optim.step() # w.data = w.data - w.grade * lr,  bias\n",
    "    # 5. 파라미터 grad 값 초기화\n",
    "    optim.zero_grad()\n",
    "    # log\n",
    "    if epoch % 100 == 0 or epoch == 999:\n",
    "        print(f\"[{epoch:03d}/{1000}] - loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 입력, 다중 출력\n",
    "- 다중입력: Feature가 여러개인 경우\n",
    "- 다중출력: Output 결과가 여러개인 경우\n",
    "\n",
    "다음 가상 데이터를 이용해 사과와 오렌지 수확량을 예측하는 선형회귀 모델을 정의한다.  \n",
    "[참조](https://www.kaggle.com/code/aakashns/pytorch-basics-linear-regression-from-scratch)\n",
    "\n",
    "\n",
    "|온도(F)|강수량(mm)|습도(%)|사과생산량(ton)|오렌지생산량|\n",
    "|-|-|-|-:|-:|\n",
    "|73|67|43|56|70|\n",
    "|91|88|64|81|101|\n",
    "|87|134|58|119|133|\n",
    "|102|43|37|22|37|\n",
    "|69|96|70|103|119|\n",
    "\n",
    "```\n",
    "사과수확량  = w11 * 온도 + w12 * 강수량 + w13 * 습도 + b1\n",
    "오렌지수확량 = w21 * 온도 + w22 * 강수량 + w23 *습도 + b2\n",
    "```\n",
    "\n",
    "- `온도`, `강수량`, `습도` 값이 **사과**와, **오렌지 수확량**에 어느정도 영향을 주는지 가중치를 찾는다.\n",
    "    - 모델은 사과의 수확량, 오렌지의 수확량 **두개의 예측결과를 출력**해야 한다.\n",
    "    - 사과에 대해 예측하기 위한 weight 3개와 오렌지에 대해 예측하기 위한 weight 3개 이렇게 두 묶음, 총 6개의 weight를 정의하고 학습을 통해 가장 적당한 값을 찾는다.\n",
    "        - `개별 과일를 예측하기 위한 weight들 @ feature들` 의 계산 결과를  **Node, Unit, Neuron** 이라고 한다.\n",
    "        - 두 과일에 대한 Unit들을 묶어서 **Layer** 라고 한다.\n",
    "- 목적은 우리가 수집한 train 데이터셋을 이용해 **정확한 예측을 위한 weight와 bias 들**을 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset\n",
    "- Train data는 feature(input)와 target(output) 각각 2개의 행렬로 구성한다.\n",
    "- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다. 이 문제에서는 `온도, 강수량, 습도` 세개의 변수를 가진다.\n",
    "- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 구성한다. 이 문제에서 예측할 항목은 `사과수확량, 오렌지 수확량` 2개의 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  input: 생산환경 (temp, rainfall, humidity) : (5, 3)\n",
    "environs = [\n",
    "    [73, 67, 43], \n",
    "    [91, 88, 64], \n",
    "    [87, 134, 58], \n",
    "    [102, 43, 37], \n",
    "    [69, 96, 70]\n",
    "]\n",
    "\n",
    "# Targets: 생산량 - (apples, oranges) - (5, 2)\n",
    "apple_orange_output = [\n",
    "    [56, 70], \n",
    "    [81, 101], \n",
    "    [119, 133], \n",
    "    [22, 37], \n",
    "    [103, 119]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5, 2]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(environs, dtype=torch.float32)\n",
    "y = torch.tensor(apple_orange_output, dtype=torch.float32)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight와 bias\n",
    "- weight: 각 feature들이 생산량에 영향을 주었는지의 가중치로 feature에 곱해줄 값.\n",
    "    - 사과, 오렌지의 생산량을 구해야 하므로 가중치가 두개가 된다.\n",
    "    - weight의 shape: `(2, 3)`\n",
    "- bias는 모든 feature들이 0일때 생산량이 얼마일지를 나타내는 값으로 feature와 weight간의 가중합 결과에 더해줄 값이다.\n",
    "    - 사과, 오렌지의 생산량을 구하므로 bias가 두개가 된다.\n",
    "    - bias의 shape: `(2, )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (N, 3)  weight: (3, 2)   3: feature들의 가중치개수, 2: 출력결과 개수(사과,오렌지)\n",
    "### 행렬곱:  N x 2\n",
    "weight = torch.randn(3, 2, requires_grad=True)\n",
    "bias = torch.randn(2, requires_grad=True) # 사과, 오렌지 생산량에 각각 더할 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986],\n",
      "        [ 0.4033,  0.8380]], requires_grad=True)\n",
      "tensor([-0.7193, -0.4033], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(weight)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사과생산량</th>\n",
       "      <th>오렌지생산략</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-215.089966</td>\n",
       "      <td>-16.578621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-268.612854</td>\n",
       "      <td>-18.118809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-312.205841</td>\n",
       "      <td>-89.756081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-254.666397</td>\n",
       "      <td>28.444017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-226.935593</td>\n",
       "      <td>-36.784912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        사과생산량     오렌지생산략\n",
       "0 -215.089966 -16.578621\n",
       "1 -268.612854 -18.118809\n",
       "2 -312.205841 -89.756081\n",
       "3 -254.666397  28.444017\n",
       "4 -226.935593 -36.784912"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "result = X @ weight + bias\n",
    "print(result.shape)\n",
    "### tensor.detach() : tensor객체를 gradient 계산 그래프에서 제거. (ndarray로 변경시 필요.)\n",
    "pd.DataFrame(result.detach().numpy(), columns=[\"사과생산량\", \"오렌지생산략\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model\n",
    "모델은 weights `w`와 inputs `x`의 내적(dot product)한 값에 bias `b`를 더하는 함수.\n",
    "\n",
    "$$\n",
    "\\hspace{2.5cm} X \\hspace{1.1cm} \\cdot \\hspace{1.2cm} W^T \\hspace{1.2cm}  + \\hspace{1cm} b \\hspace{2cm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{cc}\n",
    "73 & 67 & 43 \\\\\n",
    "91 & 88 & 64 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "69 & 96 & 70\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\cdot\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "w_{11} & w_{21} \\\\\n",
    "w_{12} & w_{22} \\\\\n",
    "w_{13} & w_{23}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "+\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "b_{1} & b_{2} \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "<center style=\"font-size:0.9em\">\n",
    "$w_{11},\\,w_{12},\\,w_{13}$: 사과 생산량 계산시 각 feature들(생산환경)에 곱할 가중치   <br>\n",
    "$w_{21},\\,w_{22},\\,w_{23}$: 오렌지 생산량 계산시 각 feature들(생산환경)에 곱할 가중치    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"figures/3_unit_layer.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn(3, 2, requires_grad=True)\n",
    "bias = torch.randn(2, requires_grad=True)\n",
    "# model\n",
    "def model(X):\n",
    "    return X @ weight + bias\n",
    "\n",
    "# loss 함수\n",
    "def loss_fn(pred, y):\n",
    "    # MSE\n",
    "    return torch.mean((pred - y)**2)  # total loss -> 두 값의 오차를 합한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients 계산\n",
    "- loss에 대해 weight와 bias의 gradients (미분계수)를 계산한다.\n",
    "- **Pytorch의 자동미분**을 이용한다. (**graident를 구하려는 tensor는 requires_grad=True로 설정한다.**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 최적화(Optimize)\n",
    "gradient decent 알고리즘을 이용해 loss를 줄여 모델의 추론 성능을 높인다. 이를 위해 좋은 성능을 낼 수 있도록 **경사하강법(gradient decent)** 을 이용해 weight와 bias를 update한다. \n",
    "\n",
    "1. 추론하기\n",
    "2. loss 계산하기\n",
    "3. weight와 bias에 대한 gradient계산하기\n",
    "4. 계산된 gradient에 비례한 값을 학습률을 곱해 작게 만든 뒤 wegith에서 빼서 조정한다.\n",
    "5. gradient를 0으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000/5000] - loss: 26670.974609375\n",
      "[100/5000] - loss: 321.3867492675781\n",
      "[200/5000] - loss: 127.18355560302734\n",
      "[300/5000] - loss: 66.44969177246094\n",
      "[400/5000] - loss: 44.231346130371094\n",
      "[500/5000] - loss: 33.70266342163086\n",
      "[600/5000] - loss: 27.18570327758789\n",
      "[700/5000] - loss: 22.401546478271484\n",
      "[800/5000] - loss: 18.607105255126953\n",
      "[900/5000] - loss: 15.508160591125488\n",
      "[1000/5000] - loss: 12.950907707214355\n",
      "[1100/5000] - loss: 10.833224296569824\n",
      "[1200/5000] - loss: 9.07742691040039\n",
      "[1300/5000] - loss: 7.621068000793457\n",
      "[1400/5000] - loss: 6.412935733795166\n",
      "[1500/5000] - loss: 5.410644054412842\n",
      "[1600/5000] - loss: 4.5791473388671875\n",
      "[1700/5000] - loss: 3.8892860412597656\n",
      "[1800/5000] - loss: 3.3169853687286377\n",
      "[1900/5000] - loss: 2.842167615890503\n",
      "[2000/5000] - loss: 2.448258876800537\n",
      "[2100/5000] - loss: 2.1214675903320312\n",
      "[2200/5000] - loss: 1.8503433465957642\n",
      "[2300/5000] - loss: 1.6254193782806396\n",
      "[2400/5000] - loss: 1.4388155937194824\n",
      "[2500/5000] - loss: 1.283996343612671\n",
      "[2600/5000] - loss: 1.1555569171905518\n",
      "[2700/5000] - loss: 1.0489970445632935\n",
      "[2800/5000] - loss: 0.9606021046638489\n",
      "[2900/5000] - loss: 0.8872586488723755\n",
      "[3000/5000] - loss: 0.8264206051826477\n",
      "[3100/5000] - loss: 0.7759422659873962\n",
      "[3200/5000] - loss: 0.7340624928474426\n",
      "[3300/5000] - loss: 0.6993166208267212\n",
      "[3400/5000] - loss: 0.6704936027526855\n",
      "[3500/5000] - loss: 0.646579384803772\n",
      "[3600/5000] - loss: 0.6267392039299011\n",
      "[3700/5000] - loss: 0.6102789044380188\n",
      "[3800/5000] - loss: 0.5966230034828186\n",
      "[3900/5000] - loss: 0.5852941274642944\n",
      "[4000/5000] - loss: 0.5758976936340332\n",
      "[4100/5000] - loss: 0.5681020021438599\n",
      "[4200/5000] - loss: 0.5616306066513062\n",
      "[4300/5000] - loss: 0.5562626123428345\n",
      "[4400/5000] - loss: 0.5518091917037964\n",
      "[4500/5000] - loss: 0.548114538192749\n",
      "[4600/5000] - loss: 0.545049786567688\n",
      "[4700/5000] - loss: 0.5425076484680176\n",
      "[4800/5000] - loss: 0.5403991937637329\n",
      "[4900/5000] - loss: 0.5386499166488647\n",
      "[4999/5000] - loss: 0.5372079610824585\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "lr = 0.00001  # 1e-5\n",
    "for epoch in range(epochs):\n",
    "    # 1. 추론\n",
    "    pred = model(X)\n",
    "    # 2. loss 계산\n",
    "    loss = loss_fn(pred, y)\n",
    "    # 3. gradient 계산\n",
    "    loss.backward()\n",
    "    # 4. update\n",
    "    weight.data = weight.data - weight.grad * lr\n",
    "    bias.data = bias.data - bias.grad * lr\n",
    "    # 5. grad값 초기화.\n",
    "    weight.grad.zero_()  # weight.grad = None\n",
    "    bias.grad.zero_()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == epochs-1:\n",
    "        print(f\"[{epoch:03d}/{epochs}] - loss: {loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.2155,  70.4138],\n",
       "        [ 82.1595, 100.5286],\n",
       "        [118.7207, 133.1138],\n",
       "        [ 21.0854,  37.0511],\n",
       "        [101.8927, 118.9826]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch built-in 모델을 사용해 Linear Regression 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[73, 67, 43], \n",
    "     [91, 88, 64], \n",
    "     [87, 134, 58], \n",
    "     [102, 43, 37], \n",
    "     [69, 96, 70]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(\n",
    "    [[56, 70], \n",
    "    [81, 101], \n",
    "    [119, 133], \n",
    "    [22, 37], \n",
    "    [103, 119]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "Pytorch는 nn.Linear 클래스를 통해 Linear Regression 모델을 제공한다.  \n",
    "nn.Linear에 입력 feature의 개수와 출력 값의 개수를 지정하면 random 값으로 초기화한 weight와 bias들을 생성해 모델을 구성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer와 Loss 함수 정의\n",
    "- torch.optim 모듈에 다양한 Optimizer 클래스가 구현되있다. 그 중에서 Adam를 사용한다.\n",
    "- torch.nn 또는 torch.nn.functional 모듈에 다양한 Loss 함수가 제공된다. 이중 mse_loss() 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model: 선형회귀 -> nn.Linear()\n",
    "model = nn.Linear(3, 2)  # 3: input feature 개수, 2: output 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.weight, model.bias\n",
    "# model.parameters() # weight/bias를 generator로 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimizer -> model의 parameter들을 넣어서 생성.\n",
    "optim = torch.optim.SGD(\n",
    "    model.parameters(), #최적화 대상 파라미터. \n",
    "    lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### loss 함수 (nn.functional의 함수, nn 클래스)\n",
    "loss_fn = nn.functional.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train\n",
    "주어진 epoch 만큼 학습하는 `fit`  함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, optim):\n",
    "    \"\"\"\n",
    "    epochs: 학습 반복 횟수\n",
    "    model: 학습시킬 대상 모델\n",
    "    loss_fn: loss function객체\n",
    "    optim: optimizer 객체\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        pred = model(inputs)\n",
    "        loss = loss_fn(pred, targets) # 추정값, 정답\n",
    "        loss.backward()\n",
    "        optim.step()   # 파라미터 업데이트\n",
    "        optim.zero_grad() # 파라미터 초기화.\n",
    "        if epoch % 100 == 0 or epoch == epochs-1:\n",
    "            print(f\"[{epoch:03d}/{epochs}] - loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000/5000] - loss: 9153.798828125\n",
      "[100/5000] - loss: 101.89180755615234\n",
      "[200/5000] - loss: 41.60322952270508\n",
      "[300/5000] - loss: 22.530529022216797\n",
      "[400/5000] - loss: 15.390551567077637\n",
      "[500/5000] - loss: 11.90361213684082\n",
      "[600/5000] - loss: 9.694496154785156\n",
      "[700/5000] - loss: 8.053624153137207\n",
      "[800/5000] - loss: 6.746133327484131\n",
      "[900/5000] - loss: 5.676537990570068\n",
      "[1000/5000] - loss: 4.7933855056762695\n",
      "[1100/5000] - loss: 4.0619001388549805\n",
      "[1200/5000] - loss: 3.45538330078125\n",
      "[1300/5000] - loss: 2.9522767066955566\n",
      "[1400/5000] - loss: 2.5349230766296387\n",
      "[1500/5000] - loss: 2.1886863708496094\n",
      "[1600/5000] - loss: 1.9014326333999634\n",
      "[1700/5000] - loss: 1.6631343364715576\n",
      "[1800/5000] - loss: 1.4654203653335571\n",
      "[1900/5000] - loss: 1.301404595375061\n",
      "[2000/5000] - loss: 1.1653225421905518\n",
      "[2100/5000] - loss: 1.052429437637329\n",
      "[2200/5000] - loss: 0.9587713479995728\n",
      "[2300/5000] - loss: 0.8810650706291199\n",
      "[2400/5000] - loss: 0.8165997266769409\n",
      "[2500/5000] - loss: 0.763117790222168\n",
      "[2600/5000] - loss: 0.7187495827674866\n",
      "[2700/5000] - loss: 0.6819407343864441\n",
      "[2800/5000] - loss: 0.6514080166816711\n",
      "[2900/5000] - loss: 0.6260701417922974\n",
      "[3000/5000] - loss: 0.6050482988357544\n",
      "[3100/5000] - loss: 0.5876117944717407\n",
      "[3200/5000] - loss: 0.5731423497200012\n",
      "[3300/5000] - loss: 0.5611426830291748\n",
      "[3400/5000] - loss: 0.551186203956604\n",
      "[3500/5000] - loss: 0.542922854423523\n",
      "[3600/5000] - loss: 0.5360721349716187\n",
      "[3700/5000] - loss: 0.5303866863250732\n",
      "[3800/5000] - loss: 0.5256666541099548\n",
      "[3900/5000] - loss: 0.5217543840408325\n",
      "[4000/5000] - loss: 0.5185094475746155\n",
      "[4100/5000] - loss: 0.5158120393753052\n",
      "[4200/5000] - loss: 0.513576865196228\n",
      "[4300/5000] - loss: 0.5117256045341492\n",
      "[4400/5000] - loss: 0.5101878643035889\n",
      "[4500/5000] - loss: 0.5089093446731567\n",
      "[4600/5000] - loss: 0.507850170135498\n",
      "[4700/5000] - loss: 0.5069716572761536\n",
      "[4800/5000] - loss: 0.5062456130981445\n",
      "[4900/5000] - loss: 0.5056418776512146\n",
      "[4999/5000] - loss: 0.5051392912864685\n"
     ]
    }
   ],
   "source": [
    "fit(5000, model, loss_fn, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.0984,  70.2448],\n",
       "        [ 82.2064, 100.6942],\n",
       "        [118.8003, 133.0059],\n",
       "        [ 21.1173,  37.0339],\n",
       "        [101.8135, 119.0906]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
