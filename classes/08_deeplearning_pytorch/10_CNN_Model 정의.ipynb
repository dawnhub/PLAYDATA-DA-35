{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1-HdRR266M6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Network 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "### Conv2d 를 생성.\n",
    "layer = nn.Conv2d(\n",
    "    in_channels=3,   # 입력 데이터의 channel 수. (입력 shape: channel, height, width -> 3차원.)\n",
    "    out_channels=5, # filter의 개수. feature map의 개수.\n",
    "    kernel_size=3,    # filter 크기 (height, width) 같은 크기면 정수로 설정. (3, 3)\n",
    "    stride=1,          # 이동 크기 (상하stride, 좌우stride) stride가 같으면 정수로 설정(1, 1) = default: 1\n",
    "    padding=0,       # padding 크기 (default: 0- no padding ('valid'))\n",
    "    # padding=\"same\" # same padding 출력의 size가 입력과 같아지게 padding 알아서 추가.\n",
    ")\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "input_data = torch.ones(1, 3, 10, 10)  # (batch_size, channel 수=in_channels, height, width)\n",
    "output = layer(input_data)\n",
    "print(output.shape)  # (1, 5, h:8, w:8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (10 - 3 + 2*0)/1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446],\n",
       "        [-0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446],\n",
       "        [-0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446],\n",
       "        [-0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446],\n",
       "        [-0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446],\n",
       "        [-0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446],\n",
       "        [-0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446],\n",
       "        [-0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### layer의 weight들 조회\n",
    "# layer.parameters() 모든 weight들과 bias를 제공하는 generator를 반환.\n",
    "layer.weight.shape\n",
    "# [5: filter개수, 3:filter채널수, 3:filter height, 3:fiter width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bias 조회 - filter당 1개\n",
    "layer.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_layer = nn.MaxPool2d(\n",
    "    kernel_size=2,   # filter 크기 (값을 추출할 영역 크기) (height, width) 같은 크기일 때는 정수.\n",
    "    stride=2,          # 좌우,  상하로 얼마씩 이동할지. (kernel_size와 동일하게 지정 해서 겹치지 않게 한다.: default(None)) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_input = torch.randn(1, 4, 4)  # (channel: 1, height: 4, width: 4)\n",
    "p_output = p_layer(p_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4206, -0.4688, -0.1028,  0.8246],\n",
       "         [ 0.2590, -1.2715,  1.3346, -0.7836],\n",
       "         [-0.5926, -0.6079,  1.4025, -0.3787],\n",
       "         [-0.7326, -0.0742, -0.0457, -1.2658]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2590,  1.3346],\n",
       "         [-0.0742,  1.4025]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_input2 = torch.randn(1, 5, 5)\n",
    "p_output2 = p_layer(p_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8441,  0.6666, -0.7669, -0.6433,  0.2646],\n",
       "         [ 0.1071, -0.6281, -0.4712,  0.8907, -0.7145],\n",
       "         [ 1.1848, -0.6985, -0.4843, -1.1012, -0.0762],\n",
       "         [ 2.8585, -1.5706,  1.3642, -1.5024,  0.1902],\n",
       "         [-0.8840, -0.4332,  1.2828, -1.0102,  0.8058]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_input2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8441, 0.8907],\n",
       "         [2.8585, 1.3642]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_layer2 = nn.MaxPool2d(\n",
    "    kernel_size=2, stride=2,\n",
    "    padding=1 # kernel size보다 작은 영역에서는 최대값을 추출 안한다.(버린다.) => padding을 추가해서 \n",
    "                    # kernel size에 맞춰서 size가 작은 영역에서도 추출하게 한다.\n",
    ")\n",
    "p_output3 = p_layer2(p_input2)\n",
    "p_output3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8441,  0.6666, -0.7669, -0.6433,  0.2646],\n",
       "         [ 0.1071, -0.6281, -0.4712,  0.8907, -0.7145],\n",
       "         [ 1.1848, -0.6985, -0.4843, -1.1012, -0.0762],\n",
       "         [ 2.8585, -1.5706,  1.3642, -1.5024,  0.1902],\n",
       "         [-0.8840, -0.4332,  1.2828, -1.0102,  0.8058]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_input2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8441,  0.6666,  0.2646],\n",
       "         [ 1.1848, -0.4712,  0.8907],\n",
       "         [ 2.8585,  1.3642,  0.8058]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_output3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from module.data import load_mnist_dataset, load_fashion_mnist_dataset\n",
    "from module.train import fit\n",
    "from module.utils import plot_fit_result\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 하이퍼파라미터 선언\n",
    "epochs = 1\n",
    "batch_size = 256\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset->DataLoader\n",
    "data_path = r\"C:\\Classes\\deeplearning\\datasets\"\n",
    "train_loader = load_mnist_dataset(data_path, batch_size)\n",
    "test_loader = load_mnist_dataset(data_path, batch_size, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv (layer) block\n",
    "#  1. Conv + ReLU + MaxPooling, (Conv + ReLU + Conv + ReLU +MaxPooling)\n",
    "#  2. Conv + BatchNorm + ReLU + MaxPooling  (BatchNorm 은 Conv, Activation 사이에 정의)\n",
    "#  3. Conv + ReLU + Dropout + MaxPooling     (Dropout activiation 다음에 정의)\n",
    "#  4. Conv + BatchNorm + ReLU + Dropout + MaxPooling\n",
    "#\n",
    "#  CNN: filter(channel-depth) 개수는 늘리고 size(height, width)는 줄여나가도록 모델 네트워크를 구성.\n",
    "####    depth 늘리는 것: Convolution Layer,   size 를 줄이것: Max Pooling \n",
    "import torch.nn as nn\n",
    "\n",
    "class MnistCNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout_rate):\n",
    "        super().__init__()\n",
    "        # conv block 단위 생성.\n",
    "        # Conv: kernel size- 3 x 3, stride=1(default), padding=same,   MaxPooling: kernel size-2, stride=1\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=\"same\"), \n",
    "            nn.BatchNorm2d(32),  # 입력 channel 수\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)            \n",
    "        )\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=\"same\"),  # stride기본값: 1\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=\"same\"), \n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)  # input: 7 X 7 -> padding 1 ( 8 X 8)\n",
    "        )\n",
    "        # # 추론기 -> Linear()\n",
    "        self.classifier = nn.Sequential(\n",
    "            ## conv output: 3차원,  linear input: 1차원\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(in_features=128*4*4, out_features=256),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 10)  # out_features=10: 정답 class 수. (0 ~ 9)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.b1(X)\n",
    "        out = self.b2(out)\n",
    "        out = self.b3(out)\n",
    "        out = self.classifier(out)\n",
    "        return  out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MnistCNNModel                            [256, 10]                 --\n",
       "├─Sequential: 1-1                        [256, 32, 14, 14]         --\n",
       "│    └─Conv2d: 2-1                       [256, 32, 28, 28]         320\n",
       "│    └─BatchNorm2d: 2-2                  [256, 32, 28, 28]         64\n",
       "│    └─ReLU: 2-3                         [256, 32, 28, 28]         --\n",
       "│    └─Dropout: 2-4                      [256, 32, 28, 28]         --\n",
       "│    └─MaxPool2d: 2-5                    [256, 32, 14, 14]         --\n",
       "├─Sequential: 1-2                        [256, 64, 7, 7]           --\n",
       "│    └─Conv2d: 2-6                       [256, 64, 14, 14]         18,496\n",
       "│    └─BatchNorm2d: 2-7                  [256, 64, 14, 14]         128\n",
       "│    └─ReLU: 2-8                         [256, 64, 14, 14]         --\n",
       "│    └─Dropout: 2-9                      [256, 64, 14, 14]         --\n",
       "│    └─MaxPool2d: 2-10                   [256, 64, 7, 7]           --\n",
       "├─Sequential: 1-3                        [256, 128, 4, 4]          --\n",
       "│    └─Conv2d: 2-11                      [256, 128, 7, 7]          73,856\n",
       "│    └─BatchNorm2d: 2-12                 [256, 128, 7, 7]          256\n",
       "│    └─ReLU: 2-13                        [256, 128, 7, 7]          --\n",
       "│    └─Dropout: 2-14                     [256, 128, 7, 7]          --\n",
       "│    └─MaxPool2d: 2-15                   [256, 128, 4, 4]          --\n",
       "├─Sequential: 1-4                        [256, 10]                 --\n",
       "│    └─Flatten: 2-16                     [256, 2048]               --\n",
       "│    └─Linear: 2-17                      [256, 256]                524,544\n",
       "│    └─ReLU: 2-18                        [256, 256]                --\n",
       "│    └─Dropout: 2-19                     [256, 256]                --\n",
       "│    └─Linear: 2-20                      [256, 10]                 2,570\n",
       "==========================================================================================\n",
       "Total params: 620,234\n",
       "Trainable params: 620,234\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.05\n",
       "==========================================================================================\n",
       "Input size (MB): 0.80\n",
       "Forward/backward pass size (MB): 180.38\n",
       "Params size (MB): 2.48\n",
       "Estimated Total Size (MB): 183.66\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_mnist_model = MnistCNNModel(0.5)\n",
    "summary(cnn_mnist_model, (batch_size, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MnistCNNModel(\n",
      "  (b1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (b2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (b3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(cnn_mnist_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = MnistCNNModel(0.3).to(device)\n",
    "# loss 함수\n",
    "loss_fn = nn.CrossEntropyLoss() # 다중 분류\n",
    "# optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/1] - Train loss: 0.29476 Train Accucracy: 0.92820 || Validation Loss: 0.28594 Validation Accuracy: 0.93030\n",
      "====================================================================================================\n",
      "<<<<<<<저장: 1 - 이전 : inf, 현재: 0.2859438624233007\n",
      "49.173094272613525 초\n"
     ]
    }
   ],
   "source": [
    "# fit()\n",
    "save_path = \"saved_models/mnist_cnn_model.pth\"\n",
    "result = fit(train_loader, test_loader, model, loss_fn, optimizer, epochs, \n",
    "               save_model_path=save_path, device=device, mode=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### colab에서 학습한 모델을 이용해서 검증 및 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from module.data import load_mnist_dataset\n",
    "from module.train import test_multi_classification\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistCNNModel(\n",
       "  (b1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (b2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (b3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 loading\n",
    "cnn_model = torch.load('saved_models/mnist_cnn_model.pth', \n",
    "                                map_location=torch.device('cpu'))  # cuda 에서 학습 모델을 cpu에서 사용하도록 설정\n",
    "cnn_model = cnn_model.to(device)\n",
    "cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.039898493385408074, Accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "### test set loading 후 모델 평가\n",
    "test_loader = load_mnist_dataset(r\"C:\\Classes\\deeplearning\\datasets\", \n",
    "                                           batch_size=256, is_train=False)\n",
    "loss, acc = test_multi_classification(test_loader, cnn_model, nn.CrossEntropyLoss(), \"cpu\")\n",
    "print(f\"Loss: {loss}, Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_img/num\\\\eight.png',\n",
       " 'test_img/num\\\\eight2.png',\n",
       " 'test_img/num\\\\five.png',\n",
       " 'test_img/num\\\\four.png',\n",
       " 'test_img/num\\\\one.png',\n",
       " 'test_img/num\\\\seven.png',\n",
       " 'test_img/num\\\\seven2.png',\n",
       " 'test_img/num\\\\three.png',\n",
       " 'test_img/num\\\\three2.png',\n",
       " 'test_img/num\\\\two.png',\n",
       " 'test_img/num\\\\two2.png']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### 새로운 이미지 추론\n",
    "from glob import glob\n",
    "img_path = glob('test_img/num/*.png')\n",
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 51)\n",
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 한개파일 테스트\n",
    "import cv2\n",
    "file_path = 'test_img/num\\\\eight.png'\n",
    "img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "# img = cv2.resize(img, (28, 28))\n",
    "# 전처리\n",
    "# input_data = transforms.ToTensor()(img)\n",
    "# input_data = input_data.unsqueeze(dim=0)\n",
    "tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((28, 28))\n",
    "])\n",
    "print(img.shape)\n",
    "input_data = tf(img).unsqueeze(dim=0)\n",
    "print(input_data.shape)\n",
    "# input_data.shape, input_data.max(), input_data.min()\n",
    "result = cnn_model(input_data)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_img/num\\eight.png: 8 - tensor([0.9763])\n",
      "test_img/num\\eight2.png: 8 - tensor([0.9797])\n",
      "test_img/num\\five.png: 5 - tensor([0.7668])\n",
      "test_img/num\\four.png: 4 - tensor([0.9993])\n",
      "test_img/num\\one.png: 1 - tensor([0.6340])\n",
      "test_img/num\\seven.png: 7 - tensor([0.4977])\n",
      "test_img/num\\seven2.png: 7 - tensor([0.7079])\n",
      "test_img/num\\three.png: 3 - tensor([0.9382])\n",
      "test_img/num\\three2.png: 3 - tensor([0.9778])\n",
      "test_img/num\\two.png: 2 - tensor([0.9907])\n",
      "test_img/num\\two2.png: 2 - tensor([0.3904])\n"
     ]
    }
   ],
   "source": [
    "for file_path in img_path:\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    # 전처리\n",
    "    input_data = transforms.ToTensor()(img)\n",
    "    input_data = input_data.unsqueeze(dim=0)\n",
    "    # input_data.shape, input_data.max(), input_data.min()\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = cnn_model(input_data)\n",
    "        result_proba = result.softmax(dim=-1)\n",
    "        print(f\"{file_path}: {result.argmax(dim=-1)[0]} - {result_proba.max(-1).values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2ARCyjDW66NR",
    "6bAN1wPG66NS",
    "shNUg6al66NV",
    "7xgQxAU666NZ"
   ],
   "name": "07_CNN_MNIST분류, 모델저장.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
