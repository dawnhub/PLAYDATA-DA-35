{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96733614",
   "metadata": {},
   "source": [
    "# DCGan \n",
    "Convolutional Layer 를 이용한 GAN 이미지 생성 모델\n",
    "\n",
    "- 논문: https://arxiv.org/pdf/1511.06434.pdf\n",
    "- 파이 토치 튜토리얼: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ef26d-dd8a-47ed-9894-b067691d3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3cdb4-7ef9-469e-9086-d39d3a3d6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed 설정\n",
    "seed_value = 0\n",
    "random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ef751-bd91-4794-9aa4-5366b92e5a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T12:24:34.987473Z",
     "iopub.status.busy": "2023-08-17T12:24:34.987233Z",
     "iopub.status.idle": "2023-08-17T12:24:34.990099Z",
     "shell.execute_reply": "2023-08-17T12:24:34.989678Z",
     "shell.execute_reply.started": "2023-08-17T12:24:34.987462Z"
    }
   },
   "source": [
    "# 하이퍼파라미터 변수 정의\n",
    "\n",
    "- **dataroot:** 학습데이터셋 저장 디렉토리 경로\n",
    "- **workers:** `DataLoader`로 데이터를 로드하기 위한 쓰레드 개수.\n",
    "- **batch_size:** 배치 크기. DCGAN **논문에서는 128**의 배치 크기를 사용.\n",
    "- **image_size:** 훈련에 사용되는 이미지의 크기. 여기서는 64 X 64 사용.\n",
    "- **nc:** 입력 이미지의 컬러 채널 수. 컬러일 경우 3.\n",
    "- **nz:** Latent vector 의 길이. Fake 이미지를 만들때 입력할 데이터.\n",
    "- **ngf:** 제너레이터의 레이어들을 통과한 특징 맵크기의 기본값으로 레이어 별로 이 값에 * N한 값을 out features로 설정.\n",
    "- **ndf:** 판별기의 레이어들을 통과한 특징 맵크기의 기본값으로 레이어 별로 이 값에 * N한 값을 out features로 설정.\n",
    "- **num_epochs:** Train 에폭 수입니다. 더 오래 훈련할수록 더 나은 결과를 얻을 수 있지만 시간도 훨씬 더 오래 걸린다.\n",
    "- **lr:** 훈련에 대한 학습률. DCGAN 논문에서 0.0002를 사용.\n",
    "- **beta1:** 아담 옵티마이저를 위한 베타1 하이퍼파라미터. 논문에서 0.5를 사용.\n",
    "- **ngpu:** 사용 가능한 GPU 개수. 0이면 CPU 모드에서 실행되고 0보다 크면 해당 수의 GPU에서 실행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28025381-7c9a-4ef9-a24b-769b7cea7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 변수 설정\n",
    "\n",
    "dataroot = r\"/home/kgmyh/atasets\"\n",
    "os.makedirs(dataroot, exist_ok=True)\n",
    "\n",
    "workers = os.cpu_count() \n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "nc = 3 \n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "num_epochs = 10\n",
    "lr = 0.0002\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c43406-be64-4f4f-a083-91619daec1f1",
   "metadata": {},
   "source": [
    "# 학습 데이터셋 - celeb-A face dataset\n",
    "- 유명인사들의 얼굴 사진들\n",
    "- torchvision의 built-in dataset으로 받을 수 있다.\n",
    "    - https://pytorch.org/vision/stable/generated/torchvision.datasets.CelebA.html#torchvision.datasets.CelebA\n",
    "- 다음 사이트에서도 다운로드 받을 수 있다.\n",
    "    - http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "    - 다운 받은 뒤 압축을 풀면 디렉토리구조가 다음과 같다.\n",
    "    - 이것을 ImageFolder 를 이용해 Dataset으로 구성할 수 있다.\n",
    "    ```\n",
    "      /path/to/celeba\n",
    "         -> img_align_celeba\n",
    "            -> 188242.jpg\n",
    "            -> 173822.jpg\n",
    "            -> 284702.jpg\n",
    "            -> 537394.jpg\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c72a584-9d5c-459c-9eba-90e5ce98a165",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])\n",
    "\n",
    "dataset = datasets.CelebA(root=dataroot, split=\"all\", target_type=[\"attr\", \"identity\"], download=True,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ae4d3-f00c-4d0f-9976-8e5f965e561b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 다운받은 일부 이미지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f6578-4c69-4f5e-9aa5-2e82ce2cb29f",
   "metadata": {},
   "source": [
    "- vutils.make_grid: https://pytorch.org/vision/main/generated/torchvision.utils.make_grid.html\n",
    "- 여러 이미지 Tensor 를 하나로 합친 Tensor를 반환한다.\n",
    "- Parameter \n",
    "    - **tensor (Tensor or list)**: 4D mini-batch Tensor (Batch, Channel, Height, Width) 또는 같은 크기의 이미지 리스트   \n",
    "    - **nrow (int, optional):** 한 행에 표시될 이미지의 개수. 최종 그리드의 형태는 ( Batch / nrow, nrow )가 된다. (Default : 8)   \n",
    "    - **padding (int, optional)**: 이미지 사이 간격 pdding (Default : 2)\n",
    "    - **normalize (bool, optional)**: True 일 경우, image 를 0~1 값으로 변환. (value_range 파라미터의 min, max 값을 기준) (Default : False)   \n",
    "    - **pad_value (float, optional)**: 패딩 되는 픽셀의 값 (Default : 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724e019-477b-4b58-9254-4ae1b24debeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(vutils.make_grid(real_batch[0][:64] \n",
    "                            , padding=2\n",
    "                            , normalize=True).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4117c-f872-438c-8c09-10f6a069bbab",
   "metadata": {},
   "source": [
    "# 모델 정의\n",
    "GAN 모델은 Generator와 Discriminator 두 개 모델을 정의한다.\n",
    "\n",
    "## 모델을 구성하는 Layer의 파라미터 초기화\n",
    "- DCGAN 논문에서 저자에서  모든 모델 가중치를 평균=0, 표준편차=0.02의 정규 분포에서 무작위로 초기화하도록 한다.   \n",
    "- `weights_init()` 함수는 Random값으로 초기화된 모델을 입력으로 받아 위 기준을 충족하도록 모든 convolution, convolution-transpose 및 Batch Normalization 레이어의 파라미터들을 다시 초기화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda27ee-bf5b-4b85-a750-c8ec0d1397dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 논문에 따라 레이어의 파라미터들을 초기화하는 함수.\n",
    "# nn.init.normal_(텐서, 평균, 표준편차): 텐서를 평균, 표준편차를 따르는 정규분포의 난수들로 채운다.\n",
    "# nn.init.constant_(텐서, value): 텐서를 value:float 으로 채운다.\n",
    "def weights_init(m:\"Layer\"):\n",
    "    \n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885051d7-6535-4135-b11e-650095addb6a",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "- Generator는 **Latent space Vector(잠재공간벡터) 를 입력** 받아 training image와 동일한 형태(분포)의 **이미지를 생성** 한다.\n",
    "    - Latent space Vector는 GAN의 입력데이터로 동일한 분포(보통 정규분포)의 random 값으로 구성된다. random 값이 어떻게 구성되느냐에 따라 다른 이미지가 생성된다.\n",
    "- Generator는 Strided Transpose Convolution, Batch Normalization, ReLU 로 이어지는 layer block들로 구성된다.\n",
    "    - **Strided Convolution** pooling layer를 사용하지 않고 stride를 이용해 size를 조정하는 것을 말함.\n",
    "    - **Transpose Convolution** 은 Convolution을 역으로 계산한다. 보통 Upsampling에 사용된다.\n",
    "- Generator의 최종 출력은 \\[-1, 1\\] 범위의 결과를 리턴한다. 그래서 출력 Layer의 activation 함수로 **tanh**를 사용한다.\n",
    "\n",
    "\n",
    "![paper](https://pytorch.org/tutorials/_images/dcgan_generator.png)<br>\n",
    "\\[DCGAN paper의 Generactor 구조\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea1e94",
   "metadata": {},
   "source": [
    "> ### Transpose Convolution Layer 출력 size 공식\n",
    "> - i: input 크기\n",
    "> - k: kernel 크기\n",
    "> - s: stride\n",
    "> - p: padding\n",
    "> \n",
    ">$$\n",
    "r\\_size = k + (i-1)\\times{s} - 2\\times{p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cf020-1335-4131-8f75-e1cb844f9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=nz, \n",
    "                               out_channels=ngf * 8, \n",
    "                               kernel_size=4, \n",
    "                               stride=1,\n",
    "                               padding=0, \n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20907e-2d6f-4d79-a84f-c54e8bf03cc6",
   "metadata": {},
   "source": [
    "Generator를 생성하고 `weights_init` 함수를 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4939d7-8afa-48ec-bc9f-a515ffedea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator 생성\n",
    "netG = Generator().to(device)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386563e2-712f-4b18-a320-8d7d548bd136",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "- Discriminator는 trainset의 진짜 이미지와 generator가 생성한 가짜 이미지를 분류하는 역할을 한다. 이미지를 입력받아 이진분류를 해서 진짜이미지 인지 여부의 확률값을 출력한다.\n",
    "- 모델의 구조는 Strided Convolution layer, Batch normalization, LeakyReLU 로 구성된 layer block들을 통과한 뒤 sigmoid activation 함수를 통해 최종 확률값을 출력한다.\n",
    "    - 논문에서 Activation 함수로 ReLU가 아닌 LeakyReLU를 사용한 것이 특징이다.\n",
    "    - 논문에서는 down sampling을 max pooling 이 아니라 convolution layer의 stride를 이용해 줄여 나간다.\n",
    "        - 이유는 pooling layer를 사용할 경우 convolution layer가 pooling 함수를 학습하게 되기 때문이라고 한다. (convolution layer가 입력의 특성을 찾는 것 뿐만 아니라 어떻게 max pooling에 적용해야 할지 까지 학습하게 된다.)\n",
    " \n",
    "![discriminator](figures/gan/discriminator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6a447-9ea7-4d69-8fb3-2f1a0beff5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb792a0-8205-4e1a-8579-430474a130aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator 생성\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "netD.apply(weights_init)\n",
    "\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa06fc-7057-49fb-9c84-d020a3d349c7",
   "metadata": {},
   "source": [
    "# 학습\n",
    "## Loss 함수와 Optimizers\n",
    "\n",
    "- GAN 모델의 최종 출력은 Real image인지 여부이므로 이진분류 문제이다.\n",
    "- Loss 함수는 Binary Cross Entropy loss (BCELoss) 함수를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bc9ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T16:55:45.692078Z",
     "start_time": "2023-11-01T16:55:45.677656Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005f55e-ccb7-4bc5-9bce-1d729a727945",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe6afc-0be6-476b-baef-ad10d3401e61",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cf3b7-6f53-4898-8787-227171c4bed4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_list = [] \n",
    "\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "s_all = time.time() \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    s = time.time()\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ####################################################################################\n",
    "        # (1) Update Discriminator(판별자) network Traing\n",
    "        ###################################################################################\n",
    "        netD.zero_grad()\n",
    "\n",
    "        real_cpu = data[0].to(device)\n",
    "        \n",
    "        b_size = real_cpu.size(0)\n",
    "        \n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        \n",
    "        output = netD(real_cpu).view(-1)\n",
    "        \n",
    "        errD_real = loss_fn(output, label)\n",
    "        \n",
    "        errD_real.backward()\n",
    "        \n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        \n",
    "        fake = netG(noise)  \n",
    "        \n",
    "        label.fill_(fake_label)\n",
    "        \n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        \n",
    "        errD_fake = loss_fn(output, label)\n",
    "        \n",
    "        errD_fake.backward()\n",
    "        \n",
    "        D_G_z1 = output.mean().item()\n",
    "        \n",
    "        errD = errD_real + errD_fake\n",
    "        \n",
    "        optimizerD.step()\n",
    "\n",
    "        #######################################################\n",
    "        # (2) Update Generator(생성자) network Traning\n",
    "        #######################################################\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        label.fill_(real_label) \n",
    "        \n",
    "        output = netD(fake).view(-1) \n",
    "        \n",
    "        errG = loss_fn(output, label)\n",
    "        \n",
    "        errG.backward()\n",
    "        \n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "        optimizerG.step()\n",
    "\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('[{:02d}/{}][{:04d}:/{}]\\tLoss_D: {:.4f}\\tLoss_G: {:.4f}\\tD(x): {:.4f}\\tD(G(z)): {:.4f} / {:.4f}'.format(\n",
    "                epoch+1,\n",
    "                num_epochs, \n",
    "                i,\n",
    "                len(dataloader),\n",
    "                errD.item(), \n",
    "                errG.item(), \n",
    "                D_x, \n",
    "                D_G_z1, \n",
    "                D_G_z2 \n",
    "            ))\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        if (i % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        \n",
    "    e = time.time()\n",
    "    print(f\"{epoch+1} epoch 걸린시간: {e-s}초\")\n",
    "    \n",
    "e_all = time.time()\n",
    "print(f\"총 걸린 시간: {e_all - s_all}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fc789-4cf1-4358-bd80-a7d3c31f1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습결과 시각화\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"Generator\")\n",
    "plt.plot(D_losses,label=\"Discriminator\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a11003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fake_image(idx):\n",
    "    \"\"\"학습도중 저장한 fake 이미지를 출력\"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    img = img_list[idx].permute(1,2,0)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "show_fake_image(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7783bcc-b78a-4059-aab9-f8fa392470d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain set의 이미지와 생성한 이미지 비교\n",
    "# real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(vutils.make_grid(real_batch[0][:64], padding=5, normalize=True).permute(1,2,0))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ddb1c-229a-484a-b2ca-684520e3f9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "465.455px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
